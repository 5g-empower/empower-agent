
Index: include/linux/netdevice.h
--- include/linux/netdevice.h.orig	Mon Dec  6 17:31:20 1999
+++ include/linux/netdevice.h	Tue Feb  1 21:21:25 2000
@@ -312,6 +312,44 @@
 	/* Semi-private data. Keep it at the end of device struct. */
 	struct dst_entry	*fastpath[NETDEV_FASTROUTE_HMASK+1];
 #endif
+
+	
+	/* Click: give kernel module controls of device operations */
+
+	/* is this card pollable? */
+	int			pollable;
+	int 			intr_is_on;
+	
+	/* dma ring statistics */
+	int			rx_dma_length;
+	int			tx_dma_length;
+	
+	/* turn interrupts on */
+	void			(*intr_on)(struct device *);
+	/* turn interrupts off */
+	void			(*intr_off)(struct device *);
+	
+	/* poll the device for packets: return next available packet on
+	 * receiving dma ring. Returns 0L if there are no packets */
+	/* struct sk_buff *	(*rx_poll)(struct device *); */
+	/* now rx_poll allows a number of packets to be polled at a time,
+	 * return a skbuff list, and set the number actually returned */
+	struct sk_buff *	(*rx_poll)(struct device *, int *want);
+	
+	/* refill rx dma ring. returns 0 if rx refilled, -1 if kernel cannot
+	 * allocate enough skb to refill the ring. */
+	int			(*rx_refill)(struct device *);
+	
+	/* place a skbuff on the transmit ring */
+	int			(*tx_queue)(struct sk_buff *skb, 
+	    				    struct device *dev);
+	
+	/* clean tx dma ring. returns the number of unsent packets pending on
+	 * the tx ring */
+	int			(*tx_clean)(struct device *);
+
+	/* start transmission */
+	int			(*tx_start)(struct device *dev);
 };
 
 
@@ -349,6 +387,11 @@
 extern int		unregister_netdevice(struct device *dev);
 extern int 		register_netdevice_notifier(struct notifier_block *nb);
 extern int		unregister_netdevice_notifier(struct notifier_block *nb);
+extern int 		register_net_in(struct notifier_block *nb); /* Click */
+extern int		unregister_net_in(struct notifier_block *nb); /* Click */
+extern int 		register_net_out(struct notifier_block *nb); /* Click */
+extern int		unregister_net_out(struct notifier_block *nb); /* Click */
+extern void		ptype_dispatch(struct sk_buff *skb, unsigned short type); /* Click */
 extern int		dev_new_index(void);
 extern struct device	*dev_get_by_index(int ifindex);
 extern int		dev_restart(struct device *dev);

Index: net/core/skbuff.c
--- net/core/skbuff.c.orig	Tue Jan 25 14:01:02 2000
+++ net/core/skbuff.c	Tue Jan 25 14:01:02 2000
@@ -4,7 +4,7 @@
  *	Authors:	Alan Cox <iiitac@pyr.swan.ac.uk>
  *			Florian La Roche <rzsfl@rz.uni-sb.de>
  *
- *	Version:	$Id: linux-2.2.10-polldev-patch,v 1.2 2000/02/09 18:16:41 benjie Exp $
+ *	Version:	$Id: linux-2.2.10-polldev-patch,v 1.2 2000/02/09 18:16:41 benjie Exp $
  *
  *	Fixes:	
  *		Alan Cox	:	Fixed the worst of the load balancer bugs.
@@ -256,6 +256,47 @@
 	n->destructor = NULL;
 	return n;
 }
+
+/*
+ * Click: recycle a sk_buff by resetting it. if it can be recycled, return it.
+ */
+struct sk_buff *skb_recycle(struct sk_buff *skb)
+{ 
+  if (atomic_dec_and_test(&skb->users)) { 
+
+    if (skb->list) 
+      printk(KERN_WARNING 
+	     "Warning: kfree_skb passed an skb still on a list (from %p).\n", 
+	     __builtin_return_address(0));
+	
+    dst_release(skb->dst); 
+    if(skb->destructor) skb->destructor(skb);
+    skb_headerinit(skb, NULL, 0);  /* clean state */ 
+	
+    if (!skb->cloned || atomic_dec_and_test(skb_datarefp(skb))) {
+      /* Load the data pointers. */ 
+      skb->data = skb->head; 
+      skb->tail = skb->data; 
+      /* end and truesize should have never changed */
+      /* skb->end = skb->data + skb->truesize; */
+
+      /* set up other state */ 
+      skb->len = 0; 
+      skb->is_clone = 0; 
+      skb->cloned = 0;
+    
+      atomic_set(&skb->users, 1); 
+      atomic_set(skb_datarefp(skb), 1);
+    
+      return skb;
+    } 
+    
+    else 
+      kfree_skbmem(skb);
+  }
+  return 0;
+}
+
 
 /*
  *	This is slower, and copies the whole data area 

Index: net/core/dev.c
--- net/core/dev.c.orig	Mon Dec  6 17:31:23 1999
+++ net/core/dev.c	Tue Feb  8 19:39:09 2000
@@ -145,6 +145,9 @@
  
 static struct notifier_block *netdev_chain=NULL;
 
+/* input packet handlers -- might steal packets from net_bh(). for Click. */
+static struct notifier_block *net_in_chain = 0;
+
 /*
  *	Device drivers call our routines to queue packets here. We empty the
  *	queue in the bottom half handler.
@@ -326,6 +329,8 @@
 		kfree(dev);
 		return NULL;
 	}
+	dev->pollable = 0;		/* by default, not pollable */
+	dev->intr_is_on = 1;		/* by default, interrupts are on */
 	return dev;
 }
 
@@ -503,6 +508,22 @@
 }
 
 /*
+ * Allow Click modules to ask to intercept input packets.
+ * Must add these to ../netsyms.c
+ */
+int
+register_net_in(struct notifier_block *nb)
+{
+  return(notifier_chain_register(&net_in_chain, nb));
+}
+
+int
+unregister_net_in(struct notifier_block *nb)
+{
+  return(notifier_chain_unregister(&net_in_chain, nb));
+}
+
+/*
  *	Support routine. Sends outgoing frames to any network
  *	taps currently in use.
  */
@@ -748,6 +769,8 @@
 	}
 }
 
+int (*udpcount_hook)(struct sk_buff *skb);
+
 /*
  *	Receive a packet from a device driver and queue it for the upper
  *	(protocol) levels.  It always succeeds. 
@@ -755,6 +778,9 @@
 
 void netif_rx(struct sk_buff *skb)
 {
+  if(udpcount_hook && udpcount_hook(skb))
+    return;
+
 #ifndef CONFIG_CPU_IS_SLOW
 	if(skb->stamp.tv_sec==0)
 		get_fast_time(&skb->stamp);
@@ -819,6 +845,84 @@
 }
 #endif
 
+/*
+ * Hand a packet to the ordinary Linux protocol stack.
+ * Broke this out from net_bh() so that Click can call it.
+ * Always frees the skb one way or another.
+ *
+ * skb->pkt_type needs to be set to PACKET_{BROADCAST,MULTICAST,OTHERHOST}
+ * maybe skb->mac.raw must point to ether header.
+ * skb->protocol must be set to a htons(ETHERTYPE_?).
+ * skb->data must point to the ethernet payload (e.g. the IP header).
+ * skb->nh.raw must point to the ethernet payload also.
+ */
+void
+ptype_dispatch(struct sk_buff *skb, unsigned short type)
+{
+	struct packet_type *ptype;
+	struct packet_type *pt_prev;
+
+		/*
+		 *	We got a packet ID.  Now loop over the "known protocols"
+		 * 	list. There are two lists. The ptype_all list of taps (normally empty)
+		 *	and the main protocol list which is hashed perfectly for normal protocols.
+		 */
+
+		pt_prev = NULL;
+		for (ptype = ptype_all; ptype!=NULL; ptype=ptype->next)
+		{
+			if (!ptype->dev || ptype->dev == skb->dev) {
+				if(pt_prev)
+				{
+					struct sk_buff *skb2=skb_clone(skb, GFP_ATOMIC);
+					if(skb2)
+						pt_prev->func(skb2,skb->dev, pt_prev);
+				}
+				pt_prev=ptype;
+			}
+		}
+
+		for (ptype = ptype_base[ntohs(type)&15]; ptype != NULL; ptype = ptype->next) 
+		{
+			if (ptype->type == type && (!ptype->dev || ptype->dev==skb->dev))
+			{
+				/*
+				 *	We already have a match queued. Deliver
+				 *	to it and then remember the new match
+				 */
+				if(pt_prev)
+				{
+					struct sk_buff *skb2;
+
+					skb2=skb_clone(skb, GFP_ATOMIC);
+
+					/*
+					 *	Kick the protocol handler. This should be fast
+					 *	and efficient code.
+					 */
+
+					if(skb2)
+						pt_prev->func(skb2, skb->dev, pt_prev);
+				}
+				/* Remember the current last to do */
+				pt_prev=ptype;
+			}
+		} /* End of protocol list loop */
+
+		/*
+		 *	Is there a last item to send to ?
+		 */
+
+		if(pt_prev)
+			pt_prev->func(skb, skb->dev, pt_prev);
+		/*
+		 * 	Has an unknown packet has been received ?
+		 */
+	 
+		else {
+			kfree_skb(skb);
+		}
+}
 
 /*
  *	When we are called the queue is ready to grab, the interrupts are
@@ -830,10 +934,8 @@
  
 void net_bh(void)
 {
-	struct packet_type *ptype;
-	struct packet_type *pt_prev;
-	unsigned short type;
 	unsigned long start_time = jiffies;
+	unsigned short type;
 #ifdef CONFIG_CPU_IS_SLOW
 	static unsigned long start_busy = 0;
 	static unsigned long ave_busy = 0;
@@ -932,66 +1034,19 @@
 		handle_bridge(skb, type); 
 #endif
 
-		/*
-		 *	We got a packet ID.  Now loop over the "known protocols"
-		 * 	list. There are two lists. The ptype_all list of taps (normally empty)
-		 *	and the main protocol list which is hashed perfectly for normal protocols.
-		 */
+                /* does Click want to steal this packet? */
+                if(notifier_call_chain(&net_in_chain, skb_queue_len(&backlog), skb) & NOTIFY_STOP_MASK)
+                  continue;
 
-		pt_prev = NULL;
-		for (ptype = ptype_all; ptype!=NULL; ptype=ptype->next)
-		{
-			if (!ptype->dev || ptype->dev == skb->dev) {
-				if(pt_prev)
-				{
-					struct sk_buff *skb2=skb_clone(skb, GFP_ATOMIC);
-					if(skb2)
-						pt_prev->func(skb2,skb->dev, pt_prev);
-				}
-				pt_prev=ptype;
-			}
-		}
 
-		for (ptype = ptype_base[ntohs(type)&15]; ptype != NULL; ptype = ptype->next) 
-		{
-			if (ptype->type == type && (!ptype->dev || ptype->dev==skb->dev))
-			{
-				/*
-				 *	We already have a match queued. Deliver
-				 *	to it and then remember the new match
-				 */
-				if(pt_prev)
-				{
-					struct sk_buff *skb2;
+                /*
+                 * Ordinary Linux dispatch based on packet type.
+                 * Moved into a function so Click can call it.
+                 */
+                ptype_dispatch(skb, type);
 
-					skb2=skb_clone(skb, GFP_ATOMIC);
-
-					/*
-					 *	Kick the protocol handler. This should be fast
-					 *	and efficient code.
-					 */
-
-					if(skb2)
-						pt_prev->func(skb2, skb->dev, pt_prev);
-				}
-				/* Remember the current last to do */
-				pt_prev=ptype;
-			}
-		} /* End of protocol list loop */
 
-		/*
-		 *	Is there a last item to send to ?
-		 */
 
-		if(pt_prev)
-			pt_prev->func(skb, skb->dev, pt_prev);
-		/*
-		 * 	Has an unknown packet has been received ?
-		 */
-	 
-		else {
-			kfree_skb(skb);
-		}
   	}	/* End of queue loop */
   	
   	/*

Index: net/sched/sch_generic.c
--- net/sched/sch_generic.c.orig	Mon Dec  6 17:31:26 1999
+++ net/sched/sch_generic.c	Mon Dec  6 14:33:35 1999
@@ -76,6 +76,24 @@
 	return q->q.qlen;
 }
 
+/*
+ * Click hooks to be notified when a network device
+ * is idle and could send a packet.
+ */
+static struct notifier_block *net_out_chain = 0;
+
+int
+register_net_out(struct notifier_block *nb)
+{
+  return(notifier_chain_register(&net_out_chain, nb));
+}
+
+int
+unregister_net_out(struct notifier_block *nb)
+{
+  return(notifier_chain_unregister(&net_out_chain, nb));
+}
+
 /* Scan transmission queue and kick devices.
 
    Deficiency: slow devices (ppp) and fast ones (100Mb ethernet)
@@ -93,9 +111,14 @@
 	hp = &qdisc_head.forw;
 	while ((h = *hp) != &qdisc_head) {
 		int res = -1;
+                int res1 = -1;
 		struct Qdisc *q = (struct Qdisc*)h;
 		struct device *dev = q->dev;
 
+                /* Click */
+                if(dev->tbusy == 0)
+                  res1 = notifier_call_chain(&net_out_chain, 0, dev);
+
 		while (!dev->tbusy && (res = qdisc_restart(dev)) < 0)
 			/* NOTHING */;
 
@@ -108,7 +131,7 @@
 		   No problem, we will unlink it during the next round.
 		 */
 
-		if (res == 0 && *hp == h) {
+		if (res1 == 0 && res == 0 && *hp == h) {
 			*hp = h->forw;
 			h->forw = NULL;
 			continue;

Index: net/netsyms.c
--- net/netsyms.c.orig	Mon Dec  6 17:31:21 1999
+++ net/netsyms.c	Mon Dec  6 14:33:33 1999
@@ -431,6 +431,17 @@
 EXPORT_SYMBOL(register_netdevice_notifier);
 EXPORT_SYMBOL(unregister_netdevice_notifier);
 
+/* Click */
+EXPORT_SYMBOL(register_net_in);
+EXPORT_SYMBOL(unregister_net_in);
+EXPORT_SYMBOL(register_net_out);
+EXPORT_SYMBOL(unregister_net_out);
+EXPORT_SYMBOL(ptype_dispatch);
+struct inet_protocol *inet_get_protocol(unsigned char prot);
+EXPORT_SYMBOL(inet_get_protocol);
+extern int (*udpcount_hook)(struct sk_buff *skb);
+EXPORT_SYMBOL(udpcount_hook);
+
 /* support for loadable net drivers */
 #ifdef CONFIG_NET
 EXPORT_SYMBOL(loopback_dev);

Index: drivers/net/tulip.c
--- drivers/net/tulip.c.orig	Fri Jan 28 09:33:37 2000
+++ drivers/net/tulip.c	Fri Feb  4 23:42:32 2000
@@ -327,6 +327,11 @@
 	TxFIFOUnderflow=0x20, TxJabber=0x08, TxNoBuf=0x04, TxDied=0x02, TxIntr=0x01,
 };
 
+/* 2.2.12 patch for hanging. rtm sep 8. */
+enum desc_status_bits {
+      DescOwned=0x80000000, RxDescFatalErr=0x8000, RxWholePkt=0x0300,
+};
+
 /* The Tulip Rx and Tx buffer descriptors. */
 struct tulip_rx_desc {
 	s32 status;
@@ -418,7 +423,18 @@
 static void tulip_tx_timeout(struct device *dev);
 static void tulip_init_ring(struct device *dev);
 static int tulip_start_xmit(struct sk_buff *skb, struct device *dev);
+static int tulip_tx(struct device *dev);
 static int tulip_rx(struct device *dev);
+static int tulip_tx_queue(struct sk_buff *skb, struct device *dev);
+static int tulip_tx_start(struct device *dev);
+static int tulip_rx_pending(struct device *dev);
+static int tulip_rx_refill(struct device *dev);
+static int tulip_rx_refill_poll(struct device *dev);
+static int tulip_tx_clean(struct device *dev);
+static struct sk_buff *tulip_rx_poll(struct device *dev, int *want);
+static int tulip_rx_dummy(struct device *dev);
+static void tulip_intr_on(struct device *dev);
+static void tulip_intr_off(struct device *dev);
 static void tulip_interrupt IRQ(int irq, void *dev_instance, struct pt_regs *regs);
 static int tulip_close(struct device *dev);
 static struct enet_statistics *tulip_get_stats(struct device *dev);
@@ -469,10 +485,12 @@
 			(PCI_CLASS_NETWORK_ETHERNET << 8,
 			 reverse_probe ? 0xfe - pci_index : pci_index,
 			 &pci_bus, &pci_device_fn) != PCIBIOS_SUCCESSFUL)
+		{
 			if (reverse_probe)
 				continue;
 			else
 				break;
+		}
 		pcibios_read_config_word(pci_bus, pci_device_fn,
 								 PCI_VENDOR_ID, &vendor);
 		pcibios_read_config_word(pci_bus, pci_device_fn,
@@ -683,6 +701,19 @@
 
 	dev->base_addr = ioaddr;
 	dev->irq = irq;
+	
+	/* Click - export routines that control device operations */
+	dev->pollable = 1;
+	dev->intr_is_on = 1;
+	dev->rx_dma_length = RX_RING_SIZE;
+	dev->tx_dma_length = TX_RING_SIZE;
+	dev->rx_poll = tulip_rx_poll;
+	dev->rx_refill = tulip_rx_refill_poll;
+	dev->tx_clean = tulip_tx_clean;
+	dev->tx_queue = tulip_tx_queue;
+	dev->tx_start = tulip_tx_start;
+	dev->intr_off = tulip_intr_off;
+	dev->intr_on = tulip_intr_on;
 
 	/* Make certain the data structures are quadword aligned. */
 	tp = (void *)(((long)kmalloc(sizeof(*tp), GFP_KERNEL | GFP_DMA) + 7) & ~7);
@@ -1349,6 +1380,8 @@
 
 	/* Start the chip's Tx to process setup frame. */
 	outl(tp->csr6, ioaddr + CSR6);
+	tp->csr6 |= 0xC000;	/* Click: bump up the Tx threshold 
+				   to max start with */
 	outl(tp->csr6 | 0x2000, ioaddr + CSR6);
 
 	dev->tbusy = 0;
@@ -1536,10 +1569,12 @@
 		outl(dev->if_port ? 0x0000000C : 0x00000004, ioaddr + CSR13);
 	} else {					/* Unknown chip type with no media table. */
 		if (tp->default_port == 0)
+		{
 			if (tp->mii_cnt) {
 				dev->if_port = 11;
 			} else
 				dev->if_port = 3;
+		}
 		if (media_cap[dev->if_port] & MediaIsMII) {
 			new_csr6 = 0x020E0000;
 		} else if (media_cap[dev->if_port] & MediaIsFx) {
@@ -1973,7 +2008,7 @@
 	add_timer(&tp->timer);
 }
 
-static void tulip_tx_timeout(struct device *dev)
+void tulip_tx_timeout(struct device *dev)
 {
   struct tulip_private *tp = (struct tulip_private *)dev->priv;
   long ioaddr = dev->base_addr;
@@ -2077,11 +2112,16 @@
 		tp->rx_ring[i].status = 0x80000000;	/* Owned by Tulip chip */
 		tp->rx_ring[i].length = PKT_BUF_SZ;
 		{
+		        extern unsigned tulip_recycled_skb_size;
 			/* Note the receive buffer must be longword aligned.
-			   dev_alloc_skb() provides 16 byte alignment.  But do *not*
-			   use skb_reserve() to align the IP header! */
+			   dev_alloc_skb() provides 16 byte alignment.  
+			   But do *not* use skb_reserve() to align the IP 
+			   header! */
 			struct sk_buff *skb;
 			skb = DEV_ALLOC_SKB(PKT_BUF_SZ);
+			/* Click: save the size we need for these rings */
+			if (tulip_recycled_skb_size == 0)
+			  tulip_recycled_skb_size = skb->truesize;
 			tp->rx_skbuff[i] = skb;
 			if (skb == NULL)
 				break;			/* Bad news!  */
@@ -2108,15 +2148,18 @@
 	tp->tx_ring[i-1].buffer2 = virt_to_bus(&tp->tx_ring[0]);
 }
 
+
+/* put a skbuff on tx dma queue */
 static int
-tulip_start_xmit(struct sk_buff *skb, struct device *dev)
+tulip_queue_skb(struct sk_buff *skb, struct device *dev)
 {
 	struct tulip_private *tp = (struct tulip_private *)dev->priv;
 	int entry;
 	u32 flag;
 
-	/* Block a timer-based transmit from overlapping.  This could better be
-	   done with atomic_swap(1, dev->tbusy), but set_bit() works as well. */
+	/* Block a timer-based transmit from overlapping.  This could better
+	 * be done with atomic_swap(1, dev->tbusy), but set_bit() works as
+	 * well. */
 	if (test_and_set_bit(0, (void*)&dev->tbusy) != 0) {
 		if (jiffies - dev->trans_start < TX_TIMEOUT)
 			return 1;
@@ -2132,35 +2175,76 @@
 
 	tp->tx_skbuff[entry] = skb;
 	tp->tx_ring[entry].buffer1 = virt_to_bus(skb->data);
-
-	if (tp->cur_tx - tp->dirty_tx < TX_RING_SIZE/2) {/* Typical path */
-	  flag = 0x60000000; /* No interrupt */
-	  dev->tbusy = 0;
+	
+	if (tp->cur_tx - tp->dirty_tx < TX_RING_SIZE/2) { /* Typical path */
+	    flag = 0x60000000; /* No interrupt */
+	    dev->tbusy = 0;
 	} else if (tp->cur_tx - tp->dirty_tx == TX_RING_SIZE/2) {
-	  flag = 0xe0000000; /* Tx-done intr. */
-	  dev->tbusy = 0;
+	    flag = 0xe0000000; /* Tx-done intr. */
+	    dev->tbusy = 0;
 	} else if (tp->cur_tx - tp->dirty_tx < TX_RING_SIZE - 2) {
-	  flag = 0x60000000; /* No Tx-done intr. */
-	  dev->tbusy = 0;
+	    flag = 0x60000000; /* No Tx-done intr. */
+	    dev->tbusy = 0;
 	} else {
-	  /* Leave room for set_rx_mode() to fill entries. */
-	  flag = 0xe0000000; /* Tx-done intr. */
-	  tp->tx_full = 1;
+	    /* Leave room for set_rx_mode() to fill entries. */
+	    flag = 0xe0000000; /* Tx-done intr. */
+	    tp->tx_full = 1;
 	}
-	if (entry == TX_RING_SIZE-1)
-		flag |= 0xe2000000;
+
+	if (entry == TX_RING_SIZE-1) flag |= 0xe2000000;
 
 	tp->tx_ring[entry].length = skb->len | flag;
-	tp->tx_ring[entry].status = 0x80000000;	/* Pass ownership to the chip. */
+	/* Pass ownership to the chip. */
+	tp->tx_ring[entry].status = 0x80000000;	
 	tp->cur_tx++;
-	/* Trigger an immediate transmit demand. */
-	outl(0, dev->base_addr + CSR1);
+	return 0;
+}
 
-	dev->trans_start = jiffies;
 
-	return 0;
+/* start the tulip transmit process by doing an outb */
+static int
+tulip_tx_start(struct device *dev)
+{ 
+  /* Trigger an immediate transmit demand. */ 
+  outl(0, dev->base_addr + CSR1); 
+  dev->trans_start = jiffies; 
+  return 0;
 }
 
+static int
+tulip_start_xmit(struct sk_buff *skb, struct device *dev)
+{
+  int r = tulip_queue_skb(skb, dev);
+  if (r==0) tulip_tx_start(dev);
+  return r;
+}
+
+/* click - intr stats */
+unsigned int txjabb = 0;
+unsigned int txdied = 0;
+unsigned int txunderflow = 0;
+unsigned int txnobuf = 0;
+unsigned int rxdied = 0;
+unsigned int rxnobuf = 0;
+
+
+/* rtm IPB */
+static __inline__ unsigned long long
+rtm_get_cycles(void)
+{
+    unsigned long low, high;
+    unsigned long long x;
+   
+    __asm__ __volatile__("rdtsc":"=a" (low), "=d" (high));
+    x = high;
+    x <<= 32;
+    x |= low;
+    return(x);
+}
+unsigned int rtm_tulip_calls;
+unsigned long long rtm_tulip_cycles;
+
+
 /* The interrupt handler does all of the Rx thread work and cleans up
    after the Tx thread. */
 static void tulip_interrupt IRQ(int irq, void *dev_instance, struct pt_regs *regs)
@@ -2170,6 +2254,9 @@
 #else
 	struct device *dev = (struct device *)(irq2dev_map[irq]);
 #endif
+#if 1
+    	unsigned long long c0;
+#endif
 
 	struct tulip_private *tp;
 	long ioaddr;
@@ -2193,6 +2280,12 @@
 #endif
 		return;
 	}
+
+#if 1
+    	rtm_tulip_calls++;
+    	c0 = rtm_get_cycles();
+#endif
+    
 	dev->interrupt = 1;
 #ifdef SMP_CHECK
 	tp->smp_proc_id = hard_smp_processor_id();
@@ -2201,86 +2294,36 @@
 	do {
 		csr5 = inl(ioaddr + CSR5);
 		/* Acknowledge all of the current interrupt sources ASAP. */
+#if 1
+		if ((csr5 & (NormalIntr|AbnormalIntr)) == 0)
+			break;
+#endif
+
 		outl(csr5 & 0x0001ffff, ioaddr + CSR5);
 
 		if (tulip_debug > 4)
 			printk(KERN_DEBUG "%s: interrupt  csr5=%#8.8x new csr5=%#8.8x.\n",
 				   dev->name, csr5, inl(dev->base_addr + CSR5));
 
-		if ((csr5 & (NormalIntr|AbnormalIntr)) == 0)
-			break;
+		if (!dev->intr_is_on)
+		{
+			if (csr5 & RxNoBuf) rxnobuf++;
+			if (csr5 & RxDied) rxdied++;
+			if (csr5 & TxDied) txdied++;
+			if (csr5 & TxJabber) txjabb++;
+		        if (csr5 & TxNoBuf) txnobuf++; 
+		        if (csr5 & TxFIFOUnderflow) txunderflow++; 
+		}
 
 		if (csr5 & (RxIntr | RxNoBuf))
+		{
 			work_budget -= tulip_rx(dev);
+			tulip_rx_refill(dev);
+		}
 
-		if (csr5 & (TxNoBuf | TxDied | TxIntr)) {
-			unsigned int dirty_tx;
-
-			for (dirty_tx = tp->dirty_tx; tp->cur_tx - dirty_tx > 0;
-				 dirty_tx++) {
-				int entry = dirty_tx % TX_RING_SIZE;
-				int status = tp->tx_ring[entry].status;
-
-				if (status < 0)
-					break;			/* It still hasn't been Txed */
-				/* Check for Rx filter setup frames. */
-				if (tp->tx_skbuff[entry] == NULL)
-				  continue;
-
-				if (status & 0x8000) {
-					/* There was an major error, log it. */
-#ifndef final_version
-					if (tulip_debug > 1)
-						printk(KERN_DEBUG "%s: Transmit error, Tx status %8.8x.\n",
-							   dev->name, status);
-#endif
-					tp->stats.tx_errors++;
-					if (status & 0x4104) tp->stats.tx_aborted_errors++;
-					if (status & 0x0C00) tp->stats.tx_carrier_errors++;
-					if (status & 0x0200) tp->stats.tx_window_errors++;
-					if (status & 0x0002) tp->stats.tx_fifo_errors++;
-					if ((status & 0x0080) && tp->full_duplex == 0)
-						tp->stats.tx_heartbeat_errors++;
-#ifdef ETHER_STATS
-					if (status & 0x0100) tp->stats.collisions16++;
-#endif
-				} else {
-#ifdef ETHER_STATS
-					if (status & 0x0001) tp->stats.tx_deferred++;
-#endif
-#if LINUX_VERSION_CODE > 0x20127
-					tp->stats.tx_bytes += tp->tx_ring[entry].length & 0x7ff;
-#endif
-					tp->stats.collisions += (status >> 3) & 15;
-					tp->stats.tx_packets++;
-				}
-
-				/* Free the original skb. */
-#if (LINUX_VERSION_CODE > 0x20155)
-				dev_kfree_skb(tp->tx_skbuff[entry]);
-#else
-				dev_kfree_skb(tp->tx_skbuff[entry], FREE_WRITE);
-#endif
-				tp->tx_skbuff[entry] = 0;
-			}
-
-#ifndef final_version
-			if (tp->cur_tx - dirty_tx > TX_RING_SIZE) {
-				printk(KERN_ERR "%s: Out-of-sync dirty pointer, %d vs. %d, full=%d.\n",
-					   dev->name, dirty_tx, tp->cur_tx, tp->tx_full);
-				dirty_tx += TX_RING_SIZE;
-			}
-#endif
+		if (csr5 & (TxNoBuf | TxDied | TxIntr)) { 
+			tulip_tx(dev);
 
-			if (tp->tx_full && dev->tbusy
-				&& tp->cur_tx - dirty_tx  < TX_RING_SIZE - 2) {
-				/* The ring is no longer full, clear tbusy. */
-				tp->tx_full = 0;
-				dev->tbusy = 0;
-				mark_bh(NET_BH);
-			}
-
-			tp->dirty_tx = dirty_tx;
 			if (csr5 & TxDied) {
 				if (tulip_debug > 1)
 					printk(KERN_WARNING "%s: The transmitter stopped!"
@@ -2304,8 +2347,11 @@
 				outl(tp->csr6 | 0x2002, ioaddr + CSR6);
 			}
 			if (csr5 & RxDied) {		/* Missed a Rx frame. */
+			  	unsigned csr8status = inl(ioaddr+CSR8);
+      				unsigned fifostatus = csr8status>>17;
 				tp->stats.rx_errors++;
-				tp->stats.rx_missed_errors += inl(ioaddr + CSR8) & 0xffff;
+				tp->stats.rx_missed_errors += csr8status&0xffff;
+      				tp->stats.rx_fifo_errors += fifostatus&0x7ff;
 			}
 			if (csr5 & TimerInt) {
 				printk(KERN_ERR "%s: Something Wicked happened! %8.8x.\n",
@@ -2340,122 +2386,208 @@
 		printk(KERN_DEBUG "%s: exiting interrupt, csr5=%#4.4x.\n",
 			   dev->name, inl(ioaddr + CSR5));
 
+#if 1
+    rtm_tulip_cycles += rtm_get_cycles() - c0;
+#endif
+
 	dev->interrupt = 0;
 	clear_bit(0, (void*)&tp->interrupt);
 	return;
 }
 
+/* clean up tx dma ring */
+static int
+tulip_tx(struct device *dev)
+{ 
+  struct tulip_private *tp; 
+  unsigned int dirty_tx; 
+  tp = (struct tulip_private *)dev->priv; 
+
+  for (dirty_tx = tp->dirty_tx; tp->cur_tx - dirty_tx > 0; dirty_tx++) { 
+    int entry = dirty_tx % TX_RING_SIZE; 
+    int status = tp->tx_ring[entry].status; 
+    
+    if (status < 0) break; /* It still hasn't been Txed */ 
+    
+    /* Check for Rx filter setup frames. */ 
+    if (tp->tx_skbuff[entry] == NULL) continue;
+
+    if (status & 0x8000) { 
+      /* There was an major error, log it. */
+#ifndef final_version 
+      if (tulip_debug > 1) 
+	printk(KERN_DEBUG "%s: Transmit error, Tx status %8.8x.\n", 
+	    dev->name, status);
+#endif 
+      tp->stats.tx_errors++; 
+      if (status & 0x4104) tp->stats.tx_aborted_errors++; 
+      if (status & 0x0C00) tp->stats.tx_carrier_errors++; 
+      if (status & 0x0200) tp->stats.tx_window_errors++;
+      if (status & 0x0002) tp->stats.tx_fifo_errors++;
+      if ((status & 0x0080) && tp->full_duplex == 0) 
+	tp->stats.tx_heartbeat_errors++;
+#ifdef ETHER_STATS 
+      if (status & 0x0100) tp->stats.collisions16++; 
+#endif 
+    } else {
+#ifdef ETHER_STATS 
+      if (status & 0x0001) tp->stats.tx_deferred++;
+#endif
+#if LINUX_VERSION_CODE > 0x20127 
+      tp->stats.tx_bytes += tp->tx_ring[entry].length & 0x7ff; 
+#endif 
+      tp->stats.collisions += (status >> 3) & 15; 
+      tp->stats.tx_packets++; 
+    }
+
+    /* Free the original skb. */
+#if (LINUX_VERSION_CODE > 0x20155) 
+    dev_kfree_skb(tp->tx_skbuff[entry]); 
+#else 
+    dev_kfree_skb(tp->tx_skbuff[entry], FREE_WRITE); 
+#endif
+    tp->tx_skbuff[entry] = 0; 
+  }
+
+#ifndef final_version 
+  if (tp->cur_tx - dirty_tx > TX_RING_SIZE) { 
+    printk(KERN_ERR "%s: Out-of-sync dirty pointer, %d vs. %d, full=%d.\n", 
+	dev->name, dirty_tx, tp->cur_tx, tp->tx_full); 
+    dirty_tx += TX_RING_SIZE; 
+  }
+#endif
+
+  if (tp->tx_full && dev->tbusy 
+      && tp->cur_tx - dirty_tx  < TX_RING_SIZE - 2) { 
+    /* The ring is no longer full, clear tbusy. */ 
+    tp->tx_full = 0; 
+    dev->tbusy = 0; 
+    mark_bh(NET_BH); 
+  }
+
+  tp->dirty_tx = dirty_tx; 
+  return tp->cur_tx-tp->dirty_tx;
+}
+
+
+
 static int
 tulip_rx(struct device *dev)
 {
-	struct tulip_private *tp = (struct tulip_private *)dev->priv;
-	int entry = tp->cur_rx % RX_RING_SIZE;
-	int rx_work_limit = tp->dirty_rx + RX_RING_SIZE - tp->cur_rx;
-	int work_done = 0;
-
-	if (tulip_debug > 4)
-		printk(KERN_DEBUG " In tulip_rx(), entry %d %8.8x.\n", entry,
-			   tp->rx_ring[entry].status);
-	/* If we own the next entry, it's a new packet. Send it up. */
-	while (tp->rx_ring[entry].status >= 0) {
-		s32 status = tp->rx_ring[entry].status;
+  struct tulip_private *tp = (struct tulip_private *)dev->priv;
+  int entry = tp->cur_rx % RX_RING_SIZE;
+  int work_done = 0;
+  int rx_work_limit = tp->dirty_rx + RX_RING_SIZE - tp->cur_rx;
+
+  /* If we own the next entry, it's a new packet. Send it up. */
+  while (tp->rx_ring[entry].status >= 0) { 
+    s32 status = tp->rx_ring[entry].status;
+    if (--rx_work_limit < 0) break; 
+
+    if ((status & 0x0300) != 0x0300) {
+      if ((status & 0xffff) != 0x7fff) { /* Ingore earlier buffers. */ 
+	if (tulip_debug > 1) 
+	  printk(KERN_WARNING "%s: Oversized Ethernet frame spanned " 
+	         "multiple buffers, status %8.8x!\n", dev->name, status); 
+	tp->stats.rx_length_errors++; 
+      } 
+    } 
+    
+    else if (status & 0x8000) {
+      /* There was a fatal error. */ 
+      if (tulip_debug > 2) 
+	printk(KERN_DEBUG "%s: Receive error, Rx status %8.8x.\n", 
+	    dev->name, status); 
+      tp->stats.rx_errors++; /* end of a packet.*/ 
+      if (status & 0x0890) tp->stats.rx_length_errors++; 
+      if (status & 0x0004) tp->stats.rx_frame_errors++; 
+      if (status & 0x0002) tp->stats.rx_crc_errors++; 
+      if (status & 0x0001) tp->stats.rx_fifo_errors++; 
+    } 
+    
+    else {
+      /* Omit the four octet CRC from the length. */ 
+      short pkt_len = (status >> 16) - 4; 
+      struct sk_buff *skb;
+
+      /* Check if the packet is long enough to just accept without 
+       * copying to a properly sized skbuff. */ 
+      if (pkt_len < rx_copybreak && (skb = DEV_ALLOC_SKB(pkt_len+2)) != NULL) { 
+	skb->dev = dev; 
+	skb_reserve(skb, 2);	/* 16 byte align the IP header */
+#if LINUX_VERSION_CODE < 0x10300 
+	memcpy(skb->data, tp->rx_ring[entry].buffer1, pkt_len);
+#elif LINUX_VERSION_CODE < 0x20200  || defined(__alpha__) 
+	memcpy(skb_put(skb, pkt_len), 
+	       bus_to_virt(tp->rx_ring[entry].buffer1), pkt_len);
+#else 
+	eth_copy_and_sum
+	  (skb, bus_to_virt(tp->rx_ring[entry].buffer1), pkt_len, 0); 
+	skb_put(skb, pkt_len);
+#endif 
+	work_done++; 
+      } 
+      
+      else { /* Pass up the skb already on the Rx ring. */ 
+	skb = tp->rx_skbuff[entry]; 
+	tp->rx_skbuff[entry] = NULL;
+#ifndef final_version 
+	{ 
+	  void *temp = skb_put(skb, pkt_len); 
+	  if (bus_to_virt(tp->rx_ring[entry].buffer1) != temp) 
+	    printk(KERN_ERR "%s: Internal consistency error! The " 
+		   "skbuff addresses do not match in tulip_rx:" 
+		   " %p vs. %p / %p.\n", dev->name, 
+		   bus_to_virt(tp->rx_ring[entry].buffer1), skb->head, temp); 
+	}
+#else 
+	skb_put(skb, pkt_len); 
+	work_done++;
+#endif 
+      }
+#if LINUX_VERSION_CODE > 0x10300 
+      skb->protocol = eth_type_trans(skb, dev); 
+#else 
+      skb->len = pkt_len;
+#endif 
+      netif_rx(skb); 
+      dev->last_rx = jiffies; 
+      tp->stats.rx_packets++;
+#if LINUX_VERSION_CODE > 0x20127 
+      tp->stats.rx_bytes += pkt_len;
+#endif 
+    } 
+    
+    entry = (++tp->cur_rx) % RX_RING_SIZE; 
+  }
 
-		if (--rx_work_limit < 0)
-			break;
-		if ((status & 0x0300) != 0x0300) {
-			if ((status & 0xffff) != 0x7fff) { /* Ingore earlier buffers. */
-				if (tulip_debug > 1)
-					printk(KERN_WARNING "%s: Oversized Ethernet frame spanned "
-						   "multiple buffers, status %8.8x!\n",
-						   dev->name, status);
-			  tp->stats.rx_length_errors++;
-			}
-		} else if (status & 0x8000) {
-			/* There was a fatal error. */
-			if (tulip_debug > 2)
-				printk(KERN_DEBUG "%s: Receive error, Rx status %8.8x.\n",
-					   dev->name, status);
-			tp->stats.rx_errors++; /* end of a packet.*/
-			if (status & 0x0890) tp->stats.rx_length_errors++;
-			if (status & 0x0004) tp->stats.rx_frame_errors++;
-			if (status & 0x0002) tp->stats.rx_crc_errors++;
-			if (status & 0x0001) tp->stats.rx_fifo_errors++;
-		} else {
-			/* Omit the four octet CRC from the length. */
-			short pkt_len = (status >> 16) - 4;
-			struct sk_buff *skb;
+  return work_done; 
+}
 
-			/* Check if the packet is long enough to just accept without
-			   copying to a properly sized skbuff. */
-			if (pkt_len < rx_copybreak
-				&& (skb = DEV_ALLOC_SKB(pkt_len+2)) != NULL) {
-				skb->dev = dev;
-				skb_reserve(skb, 2);	/* 16 byte align the IP header */
-#if LINUX_VERSION_CODE < 0x10300
-				memcpy(skb->data, tp->rx_ring[entry].buffer1, pkt_len);
-#elif LINUX_VERSION_CODE < 0x20200  || defined(__alpha__)
-				memcpy(skb_put(skb, pkt_len),
-					   bus_to_virt(tp->rx_ring[entry].buffer1), pkt_len);
-#else
-				eth_copy_and_sum(skb, bus_to_virt(tp->rx_ring[entry].buffer1),
-								 pkt_len, 0);
-				skb_put(skb, pkt_len);
-#endif
-				work_done++;
-			} else { 	/* Pass up the skb already on the Rx ring. */
-				skb = tp->rx_skbuff[entry];
-				tp->rx_skbuff[entry] = NULL;
-#ifndef final_version
-				{
-					void *temp = skb_put(skb, pkt_len);
-					if (bus_to_virt(tp->rx_ring[entry].buffer1) != temp)
-						printk(KERN_ERR "%s: Internal consistency error! The "
-						   "skbuff addresses do not match in tulip_rx:"
-							   " %p vs. %p / %p.\n", dev->name,
-							   bus_to_virt(tp->rx_ring[entry].buffer1),
-							   skb->head, temp);
-				}
-#else
-				skb_put(skb, pkt_len);
-#endif
-			}
-#if LINUX_VERSION_CODE > 0x10300
-			skb->protocol = eth_type_trans(skb, dev);
-#else
-			skb->len = pkt_len;
-#endif
-			netif_rx(skb);
-			dev->last_rx = jiffies;
-			tp->stats.rx_packets++;
-#if LINUX_VERSION_CODE > 0x20127
-			tp->stats.rx_bytes += pkt_len;
-#endif
-		}
-		entry = (++tp->cur_rx) % RX_RING_SIZE;
-	}
 
-	/* Refill the Rx ring buffers. */
-	for (; tp->cur_rx - tp->dirty_rx > 0; tp->dirty_rx++) {
-		entry = tp->dirty_rx % RX_RING_SIZE;
-		if (tp->rx_skbuff[entry] == NULL) {
-			struct sk_buff *skb;
-			skb = tp->rx_skbuff[entry] = DEV_ALLOC_SKB(PKT_BUF_SZ);
-			if (skb == NULL)
-				break;
-			skb->dev = dev;			/* Mark as being used by this device. */
-#if LINUX_VERSION_CODE > 0x10300
-			tp->rx_ring[entry].buffer1 = virt_to_bus(skb->tail);
-#else
-			tp->rx_ring[entry].buffer1 = virt_to_bus(skb->data);
-#endif
-			work_done++;
-		}
-		tp->rx_ring[entry].status = 0x80000000;
-	}
+int 
+tulip_rx_refill(struct device *dev)
+{
+  struct tulip_private *tp = (struct tulip_private *)dev->priv;
+  int entry;
+  struct sk_buff *skb;
 
-	return work_done;
+  /* Refill the Rx ring buffers. */
+  for (; tp->cur_rx - tp->dirty_rx > 0; tp->dirty_rx++) {
+    entry = tp->dirty_rx % RX_RING_SIZE;
+    skb = tp->rx_skbuff[entry];
+    if (tp->rx_skbuff[entry] == NULL) {
+      skb = tp->rx_skbuff[entry] = DEV_ALLOC_SKB(PKT_BUF_SZ);
+      if (skb == NULL) return -1;
+      skb->dev = dev; /* mark as being used by this device. */
+    }
+    tp->rx_ring[entry].buffer1 = virt_to_bus(skb->tail);
+    tp->rx_ring[entry].status = 0x80000000;
+  }
+  return 0;
 }
 
+
 static int
 tulip_close(struct device *dev)
 {
@@ -2478,7 +2610,12 @@
 	if (tp->chip_id == DC21040)
 		outl(0x00000004, ioaddr + CSR13);
 
-	tp->stats.rx_missed_errors += inl(ioaddr + CSR8) & 0xffff;
+	{
+  	  unsigned csr8status = inl(ioaddr+CSR8);
+       	  unsigned fifostatus = csr8status>>17;
+	  tp->stats.rx_missed_errors += csr8status&0xffff;
+          tp->stats.rx_fifo_errors += fifostatus&0x7ff;
+	}
 
 	del_timer(&tp->timer);
 
@@ -2529,9 +2666,12 @@
 	struct tulip_private *tp = (struct tulip_private *)dev->priv;
 	long ioaddr = dev->base_addr;
 
-	if (dev->start)
-		tp->stats.rx_missed_errors += inl(ioaddr + CSR8) & 0xffff;
-
+	if (dev->start) {
+  	  unsigned csr8status = inl(ioaddr+CSR8);
+          unsigned fifostatus = csr8status>>17;
+	  tp->stats.rx_missed_errors += csr8status&0xffff;
+          tp->stats.rx_fifo_errors += fifostatus&0x7ff;
+	}
 	return &tp->stats;
 }
 
@@ -2698,7 +2838,7 @@
 			/* Same setup recently queued, we need not add it. */
 		} else {
 			unsigned long flags;
-			unsigned int entry;
+			unsigned int entry, dummy = 0;
 			
 			save_flags(flags); cli();
 			entry = tp->cur_tx++ % TX_RING_SIZE;
@@ -2709,7 +2849,12 @@
 				tp->tx_ring[entry].length =
 					(entry == TX_RING_SIZE-1) ? 0x02000000 : 0;
 				tp->tx_ring[entry].buffer1 = 0;
+#if 1
+                /* race with chip, set DescOwned later */
+                dummy = entry;
+#else
 				tp->tx_ring[entry].status = 0x80000000;
+#endif
 				entry = tp->cur_tx++ % TX_RING_SIZE;
 			}
 
@@ -2724,6 +2869,10 @@
 				dev->tbusy = 1;
 				tp->tx_full = 1;
 			}
+#if 1
+            if (dummy >= 0)
+                tp->tx_ring[dummy].status = DescOwned;
+#endif
 			restore_flags(flags);
 			/* Trigger an immediate transmit demand. */
 			outl(0, ioaddr + CSR1);
@@ -2834,13 +2983,440 @@
 }
 
 #endif  /* MODULE */
+	
 
-/*
- * Local variables:
- *  SMP-compile-command: "gcc -D__SMP__ -DMODULE -D__KERNEL__ -I/usr/src/linux/net/inet -Wall -Wstrict-prototypes -O6 -c tulip.c `[ -f /usr/include/linux/modversions.h ] && echo -DMODVERSIONS`"
- *  compile-command: "gcc -DMODULE -D__KERNEL__ -I/usr/src/linux/net/inet -Wall -Wstrict-prototypes -O6 -c tulip.c `[ -f /usr/include/linux/modversions.h ] && echo -DMODVERSIONS`"
- *  c-indent-level: 4
- *  c-basic-offset: 4
- *  tab-width: 4
- * End:
- */
+
+/* Click polling support: below are options for optimization */
+
+/* gather skb allocation and freeing cycle counts, we use longs, so can't
+ * count for more than 1,000,000 packets */
+//#define SKB_CYCLES	1
+/* set to zero if don't want to do recycle, otherwise, 128 is a good number */
+#define SKB_RECYCLED	128
+// #define SKB_RECYCLED	0
+/* demand polling at 5.2us should not be used */
+#define DEMAND_POLLTX 	1
+/* prefetching of descriptor data */
+#define PREFETCH   	1
+
+
+unsigned tulip_recycled_skb_size = 0;
+struct sk_buff *tulip_recycled_skbs[SKB_RECYCLED];
+static int tulip_recycled_skb_cnt = 0;
+
+#ifdef SKB_CYCLES
+unsigned long skb_recycle_alloc_cnt = 0;
+unsigned long skb_recycle_alloc_cycles = 0;
+unsigned long skb_recycle_free_cnt = 0;
+unsigned long skb_recycle_free_cycles = 0;
+unsigned long skb_free_cnt = 0;
+unsigned long skb_free_cycles = 0;
+unsigned long skb_alloc_cnt = 0;
+unsigned long skb_alloc_cycles = 0;
+#endif
+
+int 
+tulip_rx_refill_poll(struct device *dev)
+{
+  struct tulip_private *tp = (struct tulip_private *)dev->priv;
+  int cleaned = 0;
+
+  /* Refill the Rx ring buffers. */
+  for (; tp->cur_rx - tp->dirty_rx > 0; tp->dirty_rx++) {
+    int entry = tp->dirty_rx % RX_RING_SIZE;
+   
+#ifdef PREFETCH
+    int next_entry = (entry+1) % RX_RING_SIZE;
+    volatile int next_status;
+    next_status = tp->rx_ring[next_entry].status;
+#endif
+
+    if (tp->rx_skbuff[entry] == NULL) {
+      struct sk_buff *skb = 0;
+
+      if (tulip_recycled_skb_cnt > 0) {
+#ifdef SKB_CYCLES
+	unsigned long long c0 = rtm_get_cycles();
+#endif
+        tulip_recycled_skb_cnt--;
+        skb = tulip_recycled_skbs[tulip_recycled_skb_cnt];
+        tp->rx_skbuff[entry] = skb;
+#ifdef SKB_CYCLES
+	skb_recycle_alloc_cycles += rtm_get_cycles()-c0;
+	skb_recycle_alloc_cnt++;
+#endif
+      }
+      
+      if (!skb) {
+#ifdef SKB_CYCLES
+	unsigned long long c0 = rtm_get_cycles();
+#endif
+        skb = tp->rx_skbuff[entry] = DEV_ALLOC_SKB(PKT_BUF_SZ);
+#ifdef SKB_CYCLES
+	skb_alloc_cycles += rtm_get_cycles()-c0;
+	skb_alloc_cnt++;
+#endif
+        if (skb == NULL) return -1;
+      }
+      skb->dev = dev; /* mark as being used by this device. */
+      tp->rx_ring[entry].buffer1 = virt_to_bus(skb->tail);
+      cleaned++;
+    }
+    tp->rx_ring[entry].status = 0x80000000;
+  }
+  return 0;
+}
+
+
+int 
+tulip_rx_pending(struct device *dev)
+{ 
+  /* cur_rx is the next entry free, either owned by tulip (status < 0) or
+   * owned by us (status >= 0) but unprocessed */
+  struct tulip_private *tp = (struct tulip_private *)dev->priv; 
+  int entry = tp->cur_rx % RX_RING_SIZE;
+  int n = 0;
+
+  while (tp->rx_ring[entry].status >= 0 && n <= RX_RING_SIZE) {
+    n++;
+    entry = (entry+1) % RX_RING_SIZE;
+  }
+  return n;
+}
+
+
+static struct sk_buff *
+tulip_rx_poll(struct device *dev, int *want)
+{
+  struct tulip_private *tp = (struct tulip_private *)dev->priv;
+  int rx_work_limit = tp->dirty_rx + RX_RING_SIZE - tp->cur_rx;
+  int entry = tp->cur_rx % RX_RING_SIZE;
+  struct sk_buff *skb_head, *skb_last;
+  int got = 0;
+ 
+  skb_head = skb_last = NULL;
+
+  while (tp->rx_ring[entry].status >= 0) {
+    s32 status = tp->rx_ring[entry].status;
+
+#ifdef PREFETCH
+    int next_entry = (entry+1) % RX_RING_SIZE;
+    volatile int next_status;
+    next_status = tp->rx_ring[next_entry].status;
+#endif
+
+    if (--rx_work_limit < 0 || got == *want) break; 
+
+    if ((status & 0x0300) != 0x0300) {
+      if ((status & 0xffff) != 0x7fff) { /* ignore earlier buffers. */ 
+	if (tulip_debug > 1) 
+	  printk(KERN_WARNING "%s: Oversized Ethernet frame spanned " 
+	         "multiple buffers, status %8.8x!\n", dev->name, status); 
+	tp->stats.rx_length_errors++; 
+      }
+    } else if (status & 0x8000) { /* fatal error */
+      if (tulip_debug > 2) 
+	printk(KERN_DEBUG "%s: Receive error, Rx status %8.8x.\n", 
+	    dev->name, status); 
+      tp->stats.rx_errors++; /* end of a packet.*/ 
+      if (status & 0x0890) tp->stats.rx_length_errors++; 
+      if (status & 0x0004) tp->stats.rx_frame_errors++; 
+      if (status & 0x0002) tp->stats.rx_crc_errors++; 
+      if (status & 0x0001) tp->stats.rx_fifo_errors++; 
+    }
+    
+    else {
+      /* Omit the four octet CRC from the length. */ 
+      short pkt_len = (status >> 16) - 4; 
+      
+      struct sk_buff *skb = tp->rx_skbuff[entry]; 
+      tp->rx_skbuff[entry] = NULL;
+
+      skb_put(skb, pkt_len); 
+      skb->protocol = eth_type_trans(skb, dev); 
+
+      tp->stats.rx_packets++;
+      tp->stats.rx_bytes += pkt_len;
+
+      if (got == 0) {
+	skb_head = skb;
+	skb_last = skb;
+	skb_last->next = NULL;
+	skb_last->prev = NULL;
+      } else {
+	skb_last->next = skb;
+	skb->prev = skb_last;
+	skb->next = NULL;
+	skb_last = skb;
+      }
+      got++;
+    }
+    entry = (++tp->cur_rx) % RX_RING_SIZE; 
+  }
+  dev->last_rx = jiffies; 
+  *want = got;
+  return skb_head;
+}
+
+static void
+tulip_intr_off(struct device *dev)
+{
+    long ioaddr = dev->base_addr;
+    int csr7;
+#ifdef DEMAND_POLLTX
+    int csr0; 
+#endif
+
+    csr7 = inl(ioaddr + CSR7) & ~(NormalIntr|RxNoBuf|RxIntr|TxIntr|TxNoBuf);
+    outl(csr7, ioaddr+CSR7);
+   
+#ifdef DEMAND_POLLTX
+    csr0 = inl(ioaddr + CSR0) & ~(7<<17);
+    csr0 = csr0 | (4<<17);
+    outl(csr0, ioaddr+CSR0);
+#endif
+
+    dev->intr_is_on = 0;
+}
+
+static void
+tulip_intr_on(struct device *dev)
+{
+    struct tulip_private *tp = (struct tulip_private *)dev->priv;
+    long ioaddr = dev->base_addr;
+    int csr7;
+#ifdef DEMAND_POLLTX
+    int csr0;
+#endif
+
+    dev->intr_is_on = 1;
+	
+    {
+      unsigned csr8status = inl(ioaddr+CSR8);
+      unsigned fifostatus = csr8status>>17;
+      tp->stats.rx_missed_errors += csr8status&0xffff;
+      tp->stats.rx_fifo_errors += fifostatus&0x7ff;
+    }
+
+#if 0
+    printk("tulip %s rx: %ld missed %ld fifooverrun\n", 
+      dev->name,
+      tp->stats.rx_missed_errors,
+      tp->stats.rx_fifo_errors);
+    printk("tulip %s tx: %ld errors %ld winerr %ld abrt %ld fifoudfl\n", 
+      dev->name,
+      tp->stats.tx_errors,
+      tp->stats.tx_window_errors,
+      tp->stats.tx_aborted_errors,
+      tp->stats.tx_fifo_errors);
+#endif
+
+#ifdef DEMAND_POLLTX
+    csr0 = inl(ioaddr + CSR0) & ~(7<<17);
+    outl(csr0, ioaddr+CSR0);
+#endif
+
+    csr7 = inl(ioaddr + CSR7) | (NormalIntr|RxNoBuf|RxIntr|TxIntr|TxNoBuf);
+    outl(csr7, ioaddr+CSR7);
+}
+
+
+void
+tulip_print_stats(void)
+{
+  printk("tulip: %d txdied %d rxdied %d rxnobuf %d txjabb %d txunderflow %d txnobuf\n", txdied, rxdied, rxnobuf, txjabb, txunderflow, txnobuf); 
+#ifdef SKB_CYCLES
+  printk("tulip: %ld recycle frees, %ld recycle free cycles, %ld recycle allocs, %ld recycle alloc cycles\n", skb_recycle_free_cnt, skb_recycle_free_cycles, skb_recycle_alloc_cnt, skb_recycle_alloc_cycles);
+  printk("tulip: %ld skb frees, %ld cycles, %ld skb allocs, %ld cycles\n", skb_free_cnt, skb_free_cycles, skb_alloc_cnt, skb_alloc_cycles);
+#endif
+}
+
+
+static int
+tulip_tx_queue(struct sk_buff *skb, struct device *dev)
+{
+  struct tulip_private *tp = (struct tulip_private *)dev->priv;
+  int entry;
+  u32 flag;
+
+  /* Caution: the write order is important here, set the base address
+     with the "ownership" bits last. */
+
+  /* Calculate the next Tx descriptor entry. */
+  entry = tp->cur_tx % TX_RING_SIZE;
+ 
+  /* this should never happen */
+  if (tp->tx_ring[entry].status < 0) return -1;
+
+  tp->tx_skbuff[entry] = skb;
+  tp->tx_ring[entry].buffer1 = virt_to_bus(skb->data);
+	
+  flag = 0x60000000; /* No interrupt */
+
+  if (tp->cur_tx - tp->dirty_tx < TX_RING_SIZE - 2) 
+    dev->tbusy = 0;
+  else {
+    /* Leave room for set_rx_mode() to fill entries. */
+    tp->tx_full = 1;
+    dev->tbusy = 1;
+  }
+  if (entry == TX_RING_SIZE-1) flag |= 0xe2000000;
+
+  tp->tx_ring[entry].length = skb->len | flag;
+  /* Pass ownership to the chip. */
+  tp->tx_ring[entry].status = 0x80000000;	
+  tp->cur_tx++;
+
+#ifndef DEMAND_POLLTX
+  outl(0, dev->base_addr + CSR1); 
+  dev->trans_start = jiffies; 
+#endif
+  return 0;
+}
+
+
+/* clean up tx dma ring */
+static int
+tulip_tx_clean(struct device *dev)
+{ 
+  struct tulip_private *tp; 
+  unsigned int dirty_tx; 
+  tp = (struct tulip_private *)dev->priv; 
+
+  for (dirty_tx = tp->dirty_tx; tp->cur_tx - dirty_tx > 0; dirty_tx++) { 
+    int entry = dirty_tx % TX_RING_SIZE; 
+    int status = tp->tx_ring[entry].status; 
+  
+#ifdef PREFETCH
+    int next_entry = (dirty_tx+1) % TX_RING_SIZE;
+    volatile int next_status;
+    next_status = tp->tx_ring[next_entry].status;
+#endif
+    
+    if (status < 0) break; /* It still hasn't been Txed */ 
+    
+    /* Check for Rx filter setup frames. */ 
+    if (tp->tx_skbuff[entry] == NULL) continue;
+
+    if (status & 0x8000) { 
+      /* There was an major error, log it. */
+      tp->stats.tx_errors++; 
+      if (status & 0x4104) tp->stats.tx_aborted_errors++; 
+      if (status & 0x0C00) tp->stats.tx_carrier_errors++; 
+      if (status & 0x0200) tp->stats.tx_window_errors++;
+      if (status & 0x0002) tp->stats.tx_fifo_errors++;
+      if ((status & 0x0080) && tp->full_duplex == 0) 
+	tp->stats.tx_heartbeat_errors++;
+    } else {
+      tp->stats.tx_bytes += tp->tx_ring[entry].length & 0x7ff; 
+      tp->stats.collisions += (status >> 3) & 15; 
+      tp->stats.tx_packets++; 
+    }
+
+    /* try to recycle as much skbuffs as we can */
+    if (tulip_recycled_skb_cnt < SKB_RECYCLED && 
+	tp->tx_skbuff[entry]->truesize == tulip_recycled_skb_size) {
+      
+      struct sk_buff *skb = tp->tx_skbuff[entry];
+#ifdef SKB_CYCLES
+      unsigned long c0 = rtm_get_cycles();
+#endif
+      tp->tx_skbuff[entry] = 0; 
+      if (skb_recycle(skb)) {
+	tulip_recycled_skbs[tulip_recycled_skb_cnt] = skb;
+	tulip_recycled_skb_cnt++;
+      }
+#ifdef SKB_CYCLES
+      skb_recycle_free_cycles += rtm_get_cycles()-c0;
+      skb_recycle_free_cnt++;
+#endif
+    }
+    else { 
+      /* Free the original skb. */
+#ifdef SKB_CYCLES
+      unsigned long long c0 = rtm_get_cycles();
+      skb_free_cnt++;
+#endif
+      dev_kfree_skb(tp->tx_skbuff[entry]); 
+      tp->tx_skbuff[entry] = 0; 
+#ifdef SKB_CYCLES
+      skb_free_cycles += rtm_get_cycles()-c0;
+#endif
+    }
+  }
+
+  if (tp->tx_full && dev->tbusy 
+      && tp->cur_tx - dirty_tx  < TX_RING_SIZE - 2) { 
+    /* The ring is no longer full, clear tbusy. */ 
+    tp->tx_full = 0; 
+    dev->tbusy = 0; 
+  }
+
+  tp->dirty_tx = dirty_tx; 
+  return tp->cur_tx-tp->dirty_tx;
+}
+
+
+int
+tulip_rx_dummy(struct device *dev)
+{
+  struct tulip_private *tp = (struct tulip_private *)dev->priv;
+  int rx_work_limit = tp->dirty_rx + RX_RING_SIZE - tp->cur_rx;
+  int entry = tp->cur_rx % RX_RING_SIZE;
+  int got = 0;
+ 
+  while (tp->rx_ring[entry].status >= 0) {
+    s32 status = tp->rx_ring[entry].status;
+
+#ifdef PREFETCH
+    int next_entry = (entry+1) % RX_RING_SIZE;
+    volatile int next_status;
+    next_status = tp->rx_ring[next_entry].status;
+#endif
+
+    if (--rx_work_limit < 0) break; 
+
+    if ((status & 0x0300) != 0x0300) {
+      if ((status & 0xffff) != 0x7fff) { /* ignore earlier buffers. */ 
+	if (tulip_debug > 1) 
+	  printk(KERN_WARNING "%s: Oversized Ethernet frame spanned " 
+	         "multiple buffers, status %8.8x!\n", dev->name, status); 
+	tp->stats.rx_length_errors++; 
+      }
+    } else if (status & 0x8000) { /* fatal error */
+      if (tulip_debug > 2) 
+	printk(KERN_DEBUG "%s: Receive error, Rx status %8.8x.\n", 
+	    dev->name, status); 
+      tp->stats.rx_errors++; /* end of a packet.*/ 
+      if (status & 0x0890) tp->stats.rx_length_errors++; 
+      if (status & 0x0004) tp->stats.rx_frame_errors++; 
+      if (status & 0x0002) tp->stats.rx_crc_errors++; 
+      if (status & 0x0001) tp->stats.rx_fifo_errors++; 
+    }
+
+    else if (tp->rx_skbuff[entry] != NULL) {
+      short pkt_len = (status >> 16) - 4; 
+      struct sk_buff *skb = tp->rx_skbuff[entry];
+      volatile char cc1, cc2; 
+      
+      asm volatile("");
+      cc1 = *(skb->data);
+      cc2 = *(skb->data+33);
+      asm volatile("");
+      
+      tp->rx_ring[entry].buffer1 = virt_to_bus(tp->rx_skbuff[entry]->tail);
+      asm volatile("");
+
+      got++;
+      tp->stats.rx_packets++;
+      tp->stats.rx_bytes += pkt_len;
+   
+      // flip status bit - give it to the card
+      tp->rx_ring[entry].status = 0x80000000;
+    }
+    entry = (++tp->cur_rx) % RX_RING_SIZE; 
+  }
+  dev->last_rx = jiffies; 
+  return got;
+}
+

Index: include/linux/signal.h
--- include/linux/signal.h.orig	Mon Dec  6 17:31:20 1999
+++ include/linux/signal.h	Tue Feb  1 21:20:48 2000
@@ -189,7 +189,7 @@
 		memset(&set->sig[1], 0, sizeof(long)*(_NSIG_WORDS-1));
 		break;
 	case 2: set->sig[1] = 0;
-	case 1:
+	case 1: ;
 	}
 }
 
@@ -201,7 +201,7 @@
 		memset(&set->sig[1], -1, sizeof(long)*(_NSIG_WORDS-1));
 		break;
 	case 2: set->sig[1] = -1;
-	case 1:
+	case 1: ;
 	}
 }
 

Index: include/linux/skbuff.h
--- include/linux/skbuff.h.orig	Tue Jan 25 09:40:52 2000
+++ include/linux/skbuff.h	Tue Feb  1 21:20:51 2000
@@ -145,6 +145,7 @@
 extern struct sk_buff *		skb_peek_copy(struct sk_buff_head *list);
 extern struct sk_buff *		alloc_skb(unsigned int size, int priority);
 extern struct sk_buff *		dev_alloc_skb(unsigned int size);
+extern struct sk_buff *		skb_recycle(struct sk_buff *buf);
 extern void			kfree_skbmem(struct sk_buff *skb);
 extern struct sk_buff *		skb_clone(struct sk_buff *skb, int priority);
 extern struct sk_buff *		skb_copy(struct sk_buff *skb, int priority);
