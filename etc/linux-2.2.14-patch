
Index: include/linux/netdevice.h
--- include/linux/netdevice.h.orig	Thu Mar  2 18:27:42 2000
+++ include/linux/netdevice.h	Sat Apr 22 15:31:35 2000
@@ -317,6 +317,32 @@
 	/* Semi-private data. Keep it at the end of device struct. */
 	struct dst_entry	*fastpath[NETDEV_FASTROUTE_HMASK+1];
 #endif
+	
+	/* Click: give kernel module controls of device operations */
+	int			pollable; 	/* is this card pollable? */
+	int 			intr_is_on;
+	int			rx_dma_length;	/* rx dma ring length */
+	int			tx_dma_length;	/* tx dma ring length */
+	unsigned long		rx_lock;	/* giant lock for rx ops */
+	unsigned long		tx_lock;	/* giant lock for tx ops */
+	
+	void			(*intr_on)(struct device *);
+	void			(*intr_off)(struct device *);
+	
+	/* now rx_poll allows a number of packets to be polled at a time,
+	 * return a skbuff list, and set the number actually returned */
+	struct sk_buff *	(*rx_poll)(struct device *, int *want);
+	/* refill rx dma ring. returns number of refilled skbuffs */
+	int			(*rx_refill)(struct device *);
+	/* place a skbuff on the transmit ring, returns 0 if successful, 1
+	 * otherwise */
+	int			(*tx_queue)(struct sk_buff *skb, 
+	    				    struct device *dev);
+	/* clean tx dma ring. returns the number of unsent packets pending on
+	 * the tx ring */
+	int			(*tx_clean)(struct device *);
+	/* start transmission */
+	int			(*tx_start)(struct device *dev);
 };
 
 
@@ -354,6 +380,11 @@
 extern int		unregister_netdevice(struct device *dev);
 extern int 		register_netdevice_notifier(struct notifier_block *nb);
 extern int		unregister_netdevice_notifier(struct notifier_block *nb);
+extern int 		register_net_in(struct notifier_block *nb); /* Click */
+extern int		unregister_net_in(struct notifier_block *nb); /* Click */
+extern int 		register_net_out(struct notifier_block *nb); /* Click */
+extern int		unregister_net_out(struct notifier_block *nb); /* Click */
+extern void		ptype_dispatch(struct sk_buff *skb, unsigned short type); /* Click */
 extern int		dev_new_index(void);
 extern struct device	*dev_get_by_index(int ifindex);
 extern int		dev_restart(struct device *dev);

Index: net/core/skbuff.c
--- net/core/skbuff.c.orig	Thu Mar  2 18:27:45 2000
+++ net/core/skbuff.c	Tue Feb 29 20:40:49 2000
@@ -4,7 +4,7 @@
  *	Authors:	Alan Cox <iiitac@pyr.swan.ac.uk>
  *			Florian La Roche <rzsfl@rz.uni-sb.de>
  *
- *	Version:	$Id: click-patch-1.0.5,v 1.11 2000/04/23 05:16:27 benjie Exp $
+ *	Version:	$Id: click-patch-1.0.5,v 1.11 2000/04/23 05:16:27 benjie Exp $
  *
  *	Fixes:	
  *		Alan Cox	:	Fixed the worst of the load balancer bugs.
@@ -256,6 +256,47 @@
 	n->destructor = NULL;
 	return n;
 }
+
+/*
+ * Click: recycle a sk_buff by resetting it. if it can be recycled, return it.
+ */
+struct sk_buff *skb_recycle(struct sk_buff *skb)
+{ 
+  if (atomic_dec_and_test(&skb->users)) { 
+
+    if (skb->list) 
+      printk(KERN_WARNING 
+	     "Warning: kfree_skb passed an skb still on a list (from %p).\n", 
+	     __builtin_return_address(0));
+	
+    dst_release(skb->dst); 
+    if(skb->destructor) skb->destructor(skb);
+    skb_headerinit(skb, NULL, 0);  /* clean state */ 
+	
+    if (!skb->cloned || atomic_dec_and_test(skb_datarefp(skb))) {
+      /* Load the data pointers. */ 
+      skb->data = skb->head; 
+      skb->tail = skb->data; 
+      /* end and truesize should have never changed */
+      /* skb->end = skb->data + skb->truesize; */
+
+      /* set up other state */ 
+      skb->len = 0; 
+      skb->is_clone = 0; 
+      skb->cloned = 0;
+    
+      atomic_set(&skb->users, 1); 
+      atomic_set(skb_datarefp(skb), 1);
+    
+      return skb;
+    } 
+    
+    else 
+      kfree_skbmem(skb);
+  }
+  return 0;
+}
+
 
 /*
  *	This is slower, and copies the whole data area 

Index: net/core/dev.c
--- net/core/dev.c.orig	Thu Mar  2 18:27:44 2000
+++ net/core/dev.c	Tue Feb 29 20:40:49 2000
@@ -148,6 +148,9 @@
  
 static struct notifier_block *netdev_chain=NULL;
 
+/* input packet handlers -- might steal packets from net_bh(). for Click. */
+static struct notifier_block *net_in_chain = 0;
+
 /*
  *	Device drivers call our routines to queue packets here. We empty the
  *	queue in the bottom half handler.
@@ -329,6 +332,8 @@
 		kfree(dev);
 		return NULL;
 	}
+	dev->pollable = 0;		/* by default, not pollable */
+	dev->intr_is_on = 1;		/* by default, interrupts are on */
 	return dev;
 }
 
@@ -506,6 +511,22 @@
 }
 
 /*
+ * Allow Click modules to ask to intercept input packets.
+ * Must add these to ../netsyms.c
+ */
+int
+register_net_in(struct notifier_block *nb)
+{
+  return(notifier_chain_register(&net_in_chain, nb));
+}
+
+int
+unregister_net_in(struct notifier_block *nb)
+{
+  return(notifier_chain_unregister(&net_in_chain, nb));
+}
+
+/*
  *	Support routine. Sends outgoing frames to any network
  *	taps currently in use.
  */
@@ -749,6 +770,8 @@
 	}
 }
 
+int (*udpcount_hook)(struct sk_buff *skb);
+
 /*
  *	Receive a packet from a device driver and queue it for the upper
  *	(protocol) levels.  It always succeeds. 
@@ -756,6 +779,9 @@
 
 void netif_rx(struct sk_buff *skb)
 {
+  if(udpcount_hook && udpcount_hook(skb))
+    return;
+
 #ifndef CONFIG_CPU_IS_SLOW
 	if(skb->stamp.tv_sec==0)
 		get_fast_time(&skb->stamp);
@@ -824,6 +850,86 @@
 }
 #endif
 
+
+/*
+ * Hand a packet to the ordinary Linux protocol stack.
+ * Broke this out from net_bh() so that Click can call it.
+ * Always frees the skb one way or another.
+ *
+ * skb->pkt_type needs to be set to PACKET_{BROADCAST,MULTICAST,OTHERHOST}
+ * maybe skb->mac.raw must point to ether header.
+ * skb->protocol must be set to a htons(ETHERTYPE_?).
+ * skb->data must point to the ethernet payload (e.g. the IP header).
+ * skb->nh.raw must point to the ethernet payload also.
+ */
+void
+ptype_dispatch(struct sk_buff *skb, unsigned short type)
+{
+	struct packet_type *ptype;
+	struct packet_type *pt_prev;
+
+		/*
+		 *	We got a packet ID.  Now loop over the "known protocols"
+		 * 	list. There are two lists. The ptype_all list of taps (normally empty)
+		 *	and the main protocol list which is hashed perfectly for normal protocols.
+		 */
+
+		pt_prev = NULL;
+		for (ptype = ptype_all; ptype!=NULL; ptype=ptype->next)
+		{
+			if (!ptype->dev || ptype->dev == skb->dev) {
+				if(pt_prev)
+				{
+					struct sk_buff *skb2=skb_clone(skb, GFP_ATOMIC);
+					if(skb2)
+						pt_prev->func(skb2,skb->dev, pt_prev);
+				}
+				pt_prev=ptype;
+			}
+		}
+
+		for (ptype = ptype_base[ntohs(type)&15]; ptype != NULL; ptype = ptype->next) 
+		{
+			if (ptype->type == type && (!ptype->dev || ptype->dev==skb->dev))
+			{
+				/*
+				 *	We already have a match queued. Deliver
+				 *	to it and then remember the new match
+				 */
+				if(pt_prev)
+				{
+					struct sk_buff *skb2;
+
+					skb2=skb_clone(skb, GFP_ATOMIC);
+
+					/*
+					 *	Kick the protocol handler. This should be fast
+					 *	and efficient code.
+					 */
+
+					if(skb2)
+						pt_prev->func(skb2, skb->dev, pt_prev);
+				}
+				/* Remember the current last to do */
+				pt_prev=ptype;
+			}
+		} /* End of protocol list loop */
+
+		/*
+		 *	Is there a last item to send to ?
+		 */
+
+		if(pt_prev)
+			pt_prev->func(skb, skb->dev, pt_prev);
+		/*
+		 * 	Has an unknown packet has been received ?
+		 */
+	 
+		else {
+			kfree_skb(skb);
+		}
+}
+
 /*
  *	When we are called the queue is ready to grab, the interrupts are
  *	on and hardware can interrupt and queue to the receive queue as we
@@ -836,8 +942,8 @@
 {
 	struct packet_type *ptype;
 	struct packet_type *pt_prev;
-	unsigned short type;
 	unsigned long start_time = jiffies;
+	unsigned short type;
 #ifdef CONFIG_CPU_IS_SLOW
 	static unsigned long start_busy = 0;
 	static unsigned long ave_busy = 0;
@@ -894,7 +1000,6 @@
 		}
 #endif
 
-
 #if 0
 		NET_PROFILE_SKB_PASSED(skb, net_bh_skb);
 #endif
@@ -936,66 +1041,19 @@
 		handle_bridge(skb, type); 
 #endif
 
-		/*
-		 *	We got a packet ID.  Now loop over the "known protocols"
-		 * 	list. There are two lists. The ptype_all list of taps (normally empty)
-		 *	and the main protocol list which is hashed perfectly for normal protocols.
-		 */
+                /* does Click want to steal this packet? */
+                if(notifier_call_chain(&net_in_chain, skb_queue_len(&backlog), skb) & NOTIFY_STOP_MASK)
+                  continue;
 
-		pt_prev = NULL;
-		for (ptype = ptype_all; ptype!=NULL; ptype=ptype->next)
-		{
-			if (!ptype->dev || ptype->dev == skb->dev) {
-				if(pt_prev)
-				{
-					struct sk_buff *skb2=skb_clone(skb, GFP_ATOMIC);
-					if(skb2)
-						pt_prev->func(skb2,skb->dev, pt_prev);
-				}
-				pt_prev=ptype;
-			}
-		}
 
-		for (ptype = ptype_base[ntohs(type)&15]; ptype != NULL; ptype = ptype->next) 
-		{
-			if (ptype->type == type && (!ptype->dev || ptype->dev==skb->dev))
-			{
-				/*
-				 *	We already have a match queued. Deliver
-				 *	to it and then remember the new match
-				 */
-				if(pt_prev)
-				{
-					struct sk_buff *skb2;
+                /*
+                 * Ordinary Linux dispatch based on packet type.
+                 * Moved into a function so Click can call it.
+                 */
+                ptype_dispatch(skb, type);
 
-					skb2=skb_clone(skb, GFP_ATOMIC);
-
-					/*
-					 *	Kick the protocol handler. This should be fast
-					 *	and efficient code.
-					 */
-
-					if(skb2)
-						pt_prev->func(skb2, skb->dev, pt_prev);
-				}
-				/* Remember the current last to do */
-				pt_prev=ptype;
-			}
-		} /* End of protocol list loop */
 
-		/*
-		 *	Is there a last item to send to ?
-		 */
 
-		if(pt_prev)
-			pt_prev->func(skb, skb->dev, pt_prev);
-		/*
-		 * 	Has an unknown packet has been received ?
-		 */
-	 
-		else {
-			kfree_skb(skb);
-		}
   	}	/* End of queue loop */
   	
   	/*

Index: net/sched/sch_generic.c
--- net/sched/sch_generic.c.orig	Thu Mar  2 18:27:46 2000
+++ net/sched/sch_generic.c	Tue Feb 29 20:40:51 2000
@@ -76,6 +76,24 @@
 	return q->q.qlen;
 }
 
+/*
+ * Click hooks to be notified when a network device
+ * is idle and could send a packet.
+ */
+static struct notifier_block *net_out_chain = 0;
+
+int
+register_net_out(struct notifier_block *nb)
+{
+  return(notifier_chain_register(&net_out_chain, nb));
+}
+
+int
+unregister_net_out(struct notifier_block *nb)
+{
+  return(notifier_chain_unregister(&net_out_chain, nb));
+}
+
 /* Scan transmission queue and kick devices.
 
    Deficiency: slow devices (ppp) and fast ones (100Mb ethernet)
@@ -93,9 +111,14 @@
 	hp = &qdisc_head.forw;
 	while ((h = *hp) != &qdisc_head) {
 		int res = -1;
+                int res1 = -1;
 		struct Qdisc *q = (struct Qdisc*)h;
 		struct device *dev = q->dev;
 
+                /* Click */
+                if(dev->tbusy == 0)
+                  res1 = notifier_call_chain(&net_out_chain, 0, dev);
+
 		while (!dev->tbusy && (res = qdisc_restart(dev)) < 0)
 			/* NOTHING */;
 
@@ -108,7 +131,7 @@
 		   No problem, we will unlink it during the next round.
 		 */
 
-		if (res == 0 && *hp == h) {
+		if (res1 == 0 && res == 0 && *hp == h) {
 			*hp = h->forw;
 			h->forw = NULL;
 			continue;

Index: net/netsyms.c
--- net/netsyms.c.orig	Thu Mar  2 18:23:41 2000
+++ net/netsyms.c	Thu Mar  2 17:10:47 2000
@@ -330,7 +330,6 @@
 EXPORT_SYMBOL(net_reset_timer);
 EXPORT_SYMBOL(net_delete_timer);
 EXPORT_SYMBOL(udp_prot);
-EXPORT_SYMBOL(tcp_prot);
 EXPORT_SYMBOL(tcp_openreq_cachep);
 EXPORT_SYMBOL(ipv4_specific);
 EXPORT_SYMBOL(tcp_simple_retransmit);
@@ -439,6 +438,18 @@
 EXPORT_SYMBOL(register_netdevice_notifier);
 EXPORT_SYMBOL(unregister_netdevice_notifier);
 
+/* Click */
+EXPORT_SYMBOL(register_net_in);
+EXPORT_SYMBOL(unregister_net_in);
+EXPORT_SYMBOL(register_net_out);
+EXPORT_SYMBOL(unregister_net_out);
+EXPORT_SYMBOL(ptype_dispatch);
+struct inet_protocol *inet_get_protocol(unsigned char prot);
+EXPORT_SYMBOL(inet_get_protocol);
+extern int (*udpcount_hook)(struct sk_buff *skb);
+EXPORT_SYMBOL(udpcount_hook);
+EXPORT_SYMBOL(tcp_prot);
+
 /* support for loadable net drivers */
 #ifdef CONFIG_NET
 EXPORT_SYMBOL(loopback_dev);
@@ -462,6 +473,7 @@
 EXPORT_SYMBOL(eth_copy_and_sum);
 EXPORT_SYMBOL(alloc_skb);
 EXPORT_SYMBOL(__kfree_skb);
+EXPORT_SYMBOL(skb_recycle);
 EXPORT_SYMBOL(skb_clone);
 EXPORT_SYMBOL(skb_copy);
 EXPORT_SYMBOL(netif_rx);

Index: drivers/net/tulip.c
--- drivers/net/tulip.c.orig	Thu Mar  2 18:27:40 2000
+++ drivers/net/tulip.c	Sun Apr 23 01:09:31 2000
@@ -22,9 +22,6 @@
 
 /* A few user-configurable values. */
 
-/* Maximum events (Rx packets, etc.) to handle at each interrupt. */
-static int max_interrupt_work = 25;
-
 #define MAX_UNITS 8
 /* Used to pass the full-duplex flag, etc. */
 static int full_duplex[MAX_UNITS] = {0, };
@@ -144,10 +141,6 @@
 
 #define RUN_AT(x) (jiffies + (x))
 
-#if (LINUX_VERSION_CODE >= 0x20100)
-static char kernel_version[] = UTS_RELEASE;
-#endif
-
 #if LINUX_VERSION_CODE < 0x20123
 #define hard_smp_processor_id() smp_processor_id()
 #define test_and_set_bit(val, addr) set_bit(val, addr)
@@ -561,6 +554,15 @@
 #endif
 static void set_rx_mode(struct device *dev);
 
+/* device polling stuff */
+static int tulip_tx_queue(struct sk_buff *skb, struct device *dev);
+static int tulip_tx_start(struct device *dev);
+static int tulip_rx_refill(struct device *dev);
+static int tulip_tx_clean(struct device *dev);
+static struct sk_buff *tulip_rx_poll(struct device *dev, int *want);
+static void tulip_intr_on(struct device *dev);
+static void tulip_intr_off(struct device *dev);
+
 
 
 /* A list of all installed Tulip devices. */
@@ -801,6 +803,19 @@
 
 	dev->base_addr = ioaddr;
 	dev->irq = irq;
+	
+	/* Click - export routines that control device operations */
+	dev->pollable = 1;
+	dev->intr_is_on = 1;
+	dev->rx_dma_length = RX_RING_SIZE;
+	dev->tx_dma_length = TX_RING_SIZE;
+	dev->rx_poll = tulip_rx_poll;
+	dev->rx_refill = tulip_rx_refill;
+	dev->tx_clean = tulip_tx_clean;
+	dev->tx_queue = tulip_tx_queue;
+	dev->tx_start = tulip_tx_start;
+	dev->intr_off = tulip_intr_off;
+	dev->intr_on = tulip_intr_on;
 
 	tp->pci_bus = pci_bus;
 	tp->pci_devfn = pci_devfn;
@@ -2503,10 +2518,14 @@
 	tp->rx_ring[i-1].buffer2 = virt_to_le32desc(&tp->rx_ring[0]);
 
 	for (i = 0; i < RX_RING_SIZE; i++) {
+	  	extern unsigned tulip_recycled_skb_size;
 		/* Note the receive buffer must be longword aligned.
 		   dev_alloc_skb() provides 16 byte alignment.  But do *not*
 		   use skb_reserve() to align the IP header! */
 		struct sk_buff *skb = dev_alloc_skb(PKT_BUF_SZ);
+		/* Click: save the size we need for these rings */
+		if (tulip_recycled_skb_size == 0)
+		  tulip_recycled_skb_size = skb->truesize;
 		tp->rx_skbuff[i] = skb;
 		if (skb == NULL)
 			break;
@@ -2526,19 +2545,30 @@
 	tp->tx_ring[i-1].buffer2 = virt_to_le32desc(&tp->tx_ring[0]);
 }
 
+/* start the tulip transmit process by doing an outb */
+static int
+tulip_tx_start(struct device *dev)
+{ 
+  /* Trigger an immediate transmit demand. */ 
+  outl(0, dev->base_addr + CSR1); 
+  dev->trans_start = jiffies; 
+  return 0;
+}
+
 static int
 tulip_start_xmit(struct sk_buff *skb, struct device *dev)
 {
 	struct tulip_private *tp = (struct tulip_private *)dev->priv;
 	int entry;
 	u32 flag;
-
-	/* Block a timer-based transmit from overlapping.  This could better be
-	   done with atomic_swap(1, dev->tbusy), but set_bit() works as well. */
+		
+	if (!dev->intr_is_on) {
+	    printk("tulip_start_xmit when interrupt is off\n");
+	}
+	
 	if (test_and_set_bit(0, (void*)&dev->tbusy) != 0) {
-		if (jiffies - dev->trans_start < TX_TIMEOUT)
-			return 1;
-		tulip_tx_timeout(dev);
+		if (jiffies - dev->trans_start >= TX_TIMEOUT)
+		    tulip_tx_timeout(dev);
 		return 1;
 	}
 
@@ -2567,16 +2597,39 @@
 	tp->tx_ring[entry].length = cpu_to_le32(skb->len | flag);
 	tp->tx_ring[entry].status = cpu_to_le32(DescOwned);
 	tp->cur_tx++;
-	if ( ! tp->tx_full)
-		clear_bit(0, (void*)&dev->tbusy);
-
-	dev->trans_start = jiffies;
-	/* Trigger an immediate transmit demand. */
-	outl(0, dev->base_addr + CSR1);
-
+	
+	tulip_tx_start(dev);
+	
+	if (!tp->tx_full) 
+	    clear_bit(0, (void*)&dev->tbusy);
+		
 	return 0;
 }
 
+/* polling extension - intr stats */
+unsigned int otheri = 0;
+unsigned int txdied = 0;
+unsigned int rxdied = 0;
+unsigned int txunderflow = 0;
+unsigned int pollintr = 0;
+unsigned int wastedintr = 0;
+unsigned long tulip_intr_calls = 0;
+unsigned long tulip_intr_pkts = 0;
+unsigned long long tulip_intr_cycles = 0;
+// #define TULIP_INTR_STATS 1
+
+static __inline__ unsigned long long
+tulip_get_cycles(void)
+{
+    unsigned long low, high;
+    unsigned long long x;
+    __asm__ __volatile__("rdtsc":"=a" (low), "=d" (high));
+    x = high;
+    x <<= 32;
+    x |= low;
+    return(x);
+}
+
 /* The interrupt handler does all of the Rx thread work and cleans up
    after the Tx thread. */
 static void tulip_interrupt(int irq, void *dev_instance, struct pt_regs *regs)
@@ -2593,6 +2646,11 @@
 	int maxrx = RX_RING_SIZE;
 	int maxtx = TX_RING_SIZE;
 	int maxoi = TX_RING_SIZE;
+	int first_time = 1;
+#ifdef TULIP_INTR_STATS
+    unsigned long p0;
+    unsigned long long c0;
+#endif
 
 #if defined(__i386__) && defined(SMP_CHECK)
 	if (test_and_set_bit(0, (void*)&dev->interrupt)) {
@@ -2611,25 +2669,48 @@
 #endif
 
 	tp->nir++;
+#ifdef TULIP_INTR_STATS
+    c0 = tulip_get_cycles();
+	p0 = tulip_intr_pkts;
+#endif
 
 	do {
 		csr5 = inl(ioaddr + CSR5);
+		
+		if ((csr5 & (NormalIntr|AbnormalIntr)) == 0) {
+		    if (!dev->intr_is_on) wastedintr++; 
+			if (first_time) goto out;
+			else break;
+		}
+		first_time = 0;
+		
 		/* Acknowledge all of the current interrupt sources ASAP. */
 		outl(csr5 & 0x0001ffff, ioaddr + CSR5);
+		
+		if (!dev->intr_is_on) {
+			pollintr++;
+			if (csr5 & RxDied) rxdied++;
+			if (csr5 & TxDied) txdied++;
+			if (csr5 & TxFIFOUnderflow) txunderflow++;
+			if (csr5 & (TimerInt|AbnormalIntr)) otheri++;
+			if ((csr5 & (TxDied|TimerInt|AbnormalIntr))==0)
+				goto out;
+		}
 
 		if (tulip_debug > 4)
 			printk(KERN_DEBUG "%s: interrupt  csr5=%#8.8x new csr5=%#8.8x.\n",
 				   dev->name, csr5, inl(dev->base_addr + CSR5));
 
-		if ((csr5 & (NormalIntr|AbnormalIntr)) == 0)
-			break;
-
-		if (csr5 & (RxIntr | RxNoBuf)) {
-			rx += tulip_rx(dev);
-			tulip_refill_rx(dev);
+		if ((csr5 & (RxIntr | RxNoBuf)) && dev->intr_is_on) {
+		    rx += tulip_rx(dev);
+		    tulip_refill_rx(dev);
 		}
 
-		if (csr5 & (TxNoBuf | TxDied | TxIntr | TimerInt)) {
+		if ((csr5 & (TxNoBuf | TxDied | TxIntr | TimerInt)) 
+			&& dev->intr_is_on) {
+		  	/* part of following code duplicated at the end
+			 * in tulip_tx_clean for the polling driver, changes
+			 * here should propagate to there as well */
 			unsigned int dirty_tx;
 
 			for (dirty_tx = tp->dirty_tx; tp->cur_tx - dirty_tx > 0;
@@ -2694,14 +2775,15 @@
 			}
 
 			tp->dirty_tx = dirty_tx;
-			if (csr5 & TxDied) {
-				if (tulip_debug > 2)
-					printk(KERN_WARNING "%s: The transmitter stopped."
-						   "  CSR5 is %x, CSR6 %x, new CSR6 %x.\n",
-						   dev->name, csr5, inl(ioaddr + CSR6), tp->csr6);
-				outl(tp->csr6 | 0x0002, ioaddr + CSR6);
-				outl(tp->csr6 | 0x2002, ioaddr + CSR6);
-			}
+		}
+
+		if (csr5 & TxDied) {
+			if (tulip_debug > 2)
+				printk(KERN_WARNING "%s: The transmitter stopped."
+					   "  CSR5 is %x, CSR6 %x, new CSR6 %x.\n",
+					   dev->name, csr5, inl(ioaddr + CSR6), tp->csr6);
+			outl(tp->csr6 | 0x0002, ioaddr + CSR6);
+			outl(tp->csr6 | 0x2002, ioaddr + CSR6);
 		}
 
 		/* Log errors. */
@@ -2720,8 +2802,11 @@
 				outl(0, ioaddr + CSR1);
 			}
 			if (csr5 & RxDied) {		/* Missed a Rx frame. */
+				unsigned csr8status = inl(ioaddr+CSR8);
+				unsigned fifostatus = csr8status>>17;
 				tp->stats.rx_errors++;
-				tp->stats.rx_missed_errors += inl(ioaddr + CSR8) & 0xffff;
+				tp->stats.rx_missed_errors += csr8status&0xffff;
+				tp->stats.rx_fifo_errors += fifostatus&0x7ff;
 				outl(tp->csr6 | 0x2002, ioaddr + CSR6);
 			}
 			if (csr5 & (TPLnkPass | TPLnkFail | 0x08000000)) {
@@ -2761,7 +2846,9 @@
 		}
 	} while (1);
 
-	tulip_refill_rx(dev);
+	if (dev->intr_is_on) {
+		tulip_refill_rx(dev);
+	}
 
 	/* check if we card is in suspend mode */
 	entry = tp->dirty_rx % RX_RING_SIZE;
@@ -2779,14 +2866,23 @@
 		}
 	}
 
+#if 0
 	if ((missed = inl(ioaddr + CSR8) & 0x1ffff)) {
 		tp->stats.rx_dropped += missed & 0x10000 ? 0x10000 : missed;
 	}
+#endif
 
 	if (tulip_debug > 4)
 		printk(KERN_DEBUG "%s: exiting interrupt, csr5=%#4.4x.\n",
 			   dev->name, inl(ioaddr + CSR5));
 
+out:
+#ifdef TULIP_INTR_STATS
+	if (tulip_intr_pkts > p0) {
+	    tulip_intr_calls++;
+	    tulip_intr_cycles += tulip_get_cycles() - c0;
+	}
+#endif
 #if defined(__i386__)
 	clear_bit(0, (void*)&dev->interrupt);
 #else
@@ -2905,6 +3001,9 @@
 #endif
 		}
 		received++;
+#ifdef TULIP_INTR_STATS
+		tulip_intr_pkts++;
+#endif
 		entry = (++tp->cur_rx) % RX_RING_SIZE;
 	}
 
@@ -2932,8 +3031,12 @@
 	if (tp->chip_id == DC21040)
 		outl(0x00000004, ioaddr + CSR13);
 
-	if (inl(ioaddr + CSR6) != 0xffffffff)
-		tp->stats.rx_missed_errors += inl(ioaddr + CSR8) & 0xffff;
+	if (inl(ioaddr + CSR6) != 0xffffffff) {
+		unsigned csr8status = inl(ioaddr+CSR8);
+		unsigned fifostatus = csr8status>>17;
+		tp->stats.rx_missed_errors += csr8status&0xffff;
+		tp->stats.rx_fifo_errors += fifostatus&0x7ff;
+	}
 
 	del_timer(&tp->timer);
 
@@ -2976,9 +3079,14 @@
 	struct tulip_private *tp = (struct tulip_private *)dev->priv;
 	long ioaddr = dev->base_addr;
 
-	if (dev->start)
-		tp->stats.rx_missed_errors += inl(ioaddr + CSR8) & 0xffff;
-
+	if (dev->start) {
+  		unsigned csr8status = inl(ioaddr+CSR8);
+        unsigned fifostatus = csr8status>>17;
+		unsigned missed = csr8status & 0x1ffff;
+		tp->stats.rx_dropped += missed & 0x10000 ? 0x10000 : missed;
+	  	tp->stats.rx_missed_errors += csr8status&0xffff;
+        tp->stats.rx_fifo_errors += fifostatus&0x7ff;
+	}
 	return &tp->stats;
 }
 
@@ -3175,7 +3283,7 @@
 			/* Same setup recently queued, we need not add it. */
 		} else {
 			unsigned long flags;
-			unsigned int entry;
+			unsigned int entry, dummy = 0;
 
 			save_flags(flags); cli();
 			entry = tp->cur_tx++ % TX_RING_SIZE;
@@ -3186,7 +3294,12 @@
 				tp->tx_ring[entry].length =
 					(entry == TX_RING_SIZE-1) ? cpu_to_le32(DESC_RING_WRAP) : 0;
 				tp->tx_ring[entry].buffer1 = 0;
+#if 1
+      				/* race with chip, set DescOwned later */
+		 		dummy = entry;
+#else
 				tp->tx_ring[entry].status = cpu_to_le32(DescOwned);
+#endif
 				entry = tp->cur_tx++ % TX_RING_SIZE;
 			}
 
@@ -3201,6 +3314,10 @@
 				set_bit(0, (void*)&dev->tbusy);
 				tp->tx_full = 1;
 			}
+#if 1
+            		if (dummy >= 0) 
+	      			tp->tx_ring[dummy].status = cpu_to_le32(DescOwned);
+#endif
 			restore_flags(flags);
 			/* Trigger an immediate transmit demand. */
 			outl(0, ioaddr + CSR1);
@@ -3345,3 +3462,341 @@
  *  tab-width: 4
  * End:
  */
+
+
+/*
+ * Polling Extension
+ *
+ * Most of the following functions are verbatim, or very similar to, code from
+ * the interrupt routines. They are cleaned up and tuned for polling. 
+ *
+ * Very minimal synchronization occurs: most polling functions are suppose to
+ * be used in polling mode, under which case the interrupt handler is
+ * disallowed from touching the rx and tx ring. Callers of polling functions
+ * are expected to synchronize calls to these functions themselves.
+ *
+ * dev->tbusy was used by Linux's original tulip driver to synchronize the
+ * send pkt routine (tulip_start_xmit) and timer based send. I am using it
+ * also to synchronize tx queueing.
+ */
+
+/* set to 0 if don't want to do recycle, otherwise, 128 is a good number */
+#define SKB_RECYCLED	128
+// #define SKB_RECYCLED	0
+/* demand polling */
+#define DEMAND_POLLTX 	1
+/* prefetching of descriptor data */
+#define PREFETCH   		1
+
+unsigned tulip_recycled_skb_size = 0;
+static int tulip_recycled_skb_cnt = 0;
+static unsigned long tulip_recycle_skb_lock = 0;
+static struct sk_buff *tulip_recycled_skbs[SKB_RECYCLED];
+
+/* WARNING: these functions are not reentrant! */
+static inline struct sk_buff*
+tulip_get_recycled_skb(void)
+{
+  	struct sk_buff* skb = 0;
+  	if (tulip_recycled_skb_cnt > 0) {
+	  	while (test_and_set_bit(0, (void*)&tulip_recycle_skb_lock)) {
+			while(tulip_recycle_skb_lock) 
+			  	asm volatile("" ::: "memory");
+		}
+      	if (tulip_recycled_skb_cnt > 0) {
+       		tulip_recycled_skb_cnt--;
+       		skb = tulip_recycled_skbs[tulip_recycled_skb_cnt];
+      	}
+		clear_bit(0, (void*)&tulip_recycle_skb_lock);
+	}
+	return skb;
+}
+/* tries to recycle an skb. if not successful, the skb passed in is freed */
+static inline void
+tulip_recycle_or_free_skb(struct sk_buff *skb)
+{
+   	if (skb->truesize == tulip_recycled_skb_size) {
+   		if (tulip_recycled_skb_cnt < SKB_RECYCLED) {
+	  		while (test_and_set_bit(0, (void*)&tulip_recycle_skb_lock)) {
+		  		while(tulip_recycle_skb_lock) 
+			  		asm volatile("" ::: "memory");
+			}
+   			if (tulip_recycled_skb_cnt < SKB_RECYCLED) {
+      			if (skb_recycle(skb)) {
+					tulip_recycled_skbs[tulip_recycled_skb_cnt] = skb;
+					tulip_recycled_skb_cnt++;
+      			}
+				skb = 0;
+			}
+			clear_bit(0, (void*)&tulip_recycle_skb_lock);
+    	}
+	}
+	if (skb != 0) dev_kfree_skb(skb);
+}
+
+int 
+tulip_rx_refill(struct device *dev)
+{
+  	struct tulip_private *tp = (struct tulip_private *)dev->priv;
+  	int cleaned = 0;
+	
+  	/* Refill the Rx ring buffers. */
+  	for (; tp->cur_rx - tp->dirty_rx > 0; tp->dirty_rx++) { 
+		int entry = tp->dirty_rx % RX_RING_SIZE;
+#ifdef PREFETCH
+    	int next_entry = (entry+1) % RX_RING_SIZE;
+    	volatile int next_status;
+    	next_status = tp->rx_ring[next_entry].status;
+#endif
+
+    	if (tp->rx_skbuff[entry] == NULL) {
+      		struct sk_buff *skb = tulip_get_recycled_skb();
+
+			if (skb)
+        		tp->rx_skbuff[entry] = skb;
+			else {
+        		skb = tp->rx_skbuff[entry] = dev_alloc_skb(PKT_BUF_SZ);
+        		if (skb == NULL)
+					return -1;
+      		}
+      		skb->dev = dev; /* mark as being used by this device. */
+      		tp->rx_ring[entry].buffer1 = virt_to_le32desc(skb->tail);
+      		cleaned++;
+    	}
+    	tp->rx_ring[entry].status = cpu_to_le32(DescOwned);
+  	}
+  	return 0;
+}
+
+static struct sk_buff *
+tulip_rx_poll(struct device *dev, int *want)
+{
+  	struct tulip_private *tp = (struct tulip_private *)dev->priv;
+	int rx_work_limit = tp->dirty_rx + RX_RING_SIZE - tp->cur_rx;
+  	int entry = tp->cur_rx % RX_RING_SIZE;
+  	struct sk_buff *skb_head, *skb_last;
+  	int got = 0;
+ 
+  	skb_head = skb_last = NULL;
+
+  	while (! (tp->rx_ring[entry].status & cpu_to_le32(DescOwned))) {
+    	s32 status = le32_to_cpu(tp->rx_ring[entry].status);
+#ifdef PREFETCH
+    	int next_entry = (entry+1) % RX_RING_SIZE;
+    	volatile int next_status;
+    	next_status = tp->rx_ring[next_entry].status;
+#endif
+    	if (--rx_work_limit < 0 || got == *want) break; 
+
+    	if ((status & 0x38008300) != 0x0300) {
+    		if ((status & 0x38000300) != 0x0300) {
+				/* Ignore earlier buffers. */ 
+      			if ((status & 0xffff) != 0x7fff) { 
+					if (tulip_debug > 1) 
+					  printk(KERN_WARNING "%s: Oversized Ethernet frame "
+						     "spanned " "multiple buffers, status %8.8x!\n", 
+							 dev->name, status); 
+					tp->stats.rx_length_errors++; 
+      			}
+    		} else if (status & RxDescFatalErr) {
+			  	/* There was a fatal error */
+      			if (tulip_debug > 2) 
+					printk(KERN_DEBUG "%s: Receive error, Rx status %8.8x.\n", 
+					   	   dev->name, status); 
+      			tp->stats.rx_errors++; /* end of a packet.*/ 
+      			if (status & 0x0890) tp->stats.rx_length_errors++; 
+      			if (status & 0x0004) tp->stats.rx_frame_errors++; 
+      			if (status & 0x0002) tp->stats.rx_crc_errors++; 
+      			if (status & 0x0001) tp->stats.rx_fifo_errors++; 
+    		}
+		} else {
+      		/* Omit the four octet CRC from the length. */ 
+      		short pkt_len = ((status >> 16) & 0x7ff) - 4; 
+      		struct sk_buff *skb = tp->rx_skbuff[entry]; 
+      		tp->rx_skbuff[entry] = NULL;
+
+      		skb_put(skb, pkt_len); 
+      		skb->protocol = eth_type_trans(skb, dev); 
+
+      		tp->stats.rx_packets++;
+      		tp->stats.rx_bytes += pkt_len;
+
+      		if (got == 0) {
+				skb_head = skb;
+				skb_last = skb;
+				skb_last->next = NULL;
+				skb_last->prev = NULL;
+      		} else {
+				skb_last->next = skb;
+				skb->prev = skb_last;
+				skb->next = NULL;
+				skb_last = skb;
+      		}
+      		got++;
+    	}
+    	entry = (++tp->cur_rx) % RX_RING_SIZE; 
+  	}
+  	dev->last_rx = jiffies; 
+  	*want = got;
+  	return skb_head;
+}
+
+void
+tulip_print_stats(void)
+{
+  	printk("tulip - %d poll intrs, %d wasted: %d txdied %d rxdied %d txunderflow %d otherintr\n", pollintr, wastedintr, txdied, rxdied, txunderflow, otheri); 
+	printk("tulip - %ld pkts, %ld intrs, 0x%x 0x%x cycles\n", 
+		   tulip_intr_pkts,
+		   tulip_intr_calls, 
+		   *(((int *)&tulip_intr_cycles)+1),
+		   *((int *)&tulip_intr_cycles));
+}
+
+
+static void
+tulip_intr_off(struct device *dev)
+{
+    long ioaddr = dev->base_addr;
+    int csr7;
+#ifdef DEMAND_POLLTX
+    int csr0; 
+#endif
+    csr7 = inl(ioaddr + CSR7) & ~(NormalIntr|RxNoBuf|RxIntr|TxIntr|TxNoBuf);
+    outl(csr7, ioaddr+CSR7);
+#ifdef DEMAND_POLLTX
+    csr0 = inl(ioaddr + CSR0) & ~(7<<17);
+    csr0 = csr0 | (4<<17);
+    outl(csr0, ioaddr+CSR0);
+#endif
+    dev->intr_is_on = 0;
+}
+
+static void
+tulip_intr_on(struct device *dev)
+{
+    struct tulip_private *tp = (struct tulip_private *)dev->priv;
+    long ioaddr = dev->base_addr;
+    int csr7;
+#ifdef DEMAND_POLLTX
+    int csr0;
+#endif
+    dev->intr_is_on = 1;
+    printk("tulip %s rx: %ld missed %ld fifooverrun\n", 
+           dev->name,
+           tp->stats.rx_missed_errors,
+           tp->stats.rx_fifo_errors);
+    printk("tulip %s tx: %ld errors %ld winerr %ld abrt %ld fifoudfl\n", 
+           dev->name,
+           tp->stats.tx_errors,
+           tp->stats.tx_window_errors,
+           tp->stats.tx_aborted_errors,
+           tp->stats.tx_fifo_errors);
+#ifdef DEMAND_POLLTX
+    csr0 = inl(ioaddr + CSR0) & ~(7<<17);
+    outl(csr0, ioaddr+CSR0);
+#endif
+    csr7 = inl(ioaddr + CSR7) | (NormalIntr|RxNoBuf|RxIntr|TxIntr|TxNoBuf);
+    outl(csr7, ioaddr+CSR7);
+}
+
+static int
+tulip_tx_queue(struct sk_buff *skb, struct device *dev)
+{
+  	struct tulip_private *tp = (struct tulip_private *)dev->priv;
+  	int entry;
+  	u32 flag;
+
+	if (test_and_set_bit(0, (void*)&dev->tbusy) != 0) {
+	    printk("tulip_tx_queue: reject because tbusy\n");
+		return 1;
+	}
+
+	/* Caution: the write order is important here, set the base address
+	 * with the "ownership" bits last. */
+
+  	/* Calculate the next Tx descriptor entry. */
+  	entry = tp->cur_tx % TX_RING_SIZE;
+ 
+  	tp->tx_skbuff[entry] = skb;
+  	tp->tx_ring[entry].buffer1 = virt_to_le32desc(skb->data);
+	
+  	flag = 0x60000000; /* No interrupt */
+
+  	if (tp->cur_tx - tp->dirty_tx < TX_RING_SIZE - 2) 
+	    tp->tx_full = 0;
+  	else 
+    	/* Leave room for set_rx_mode() to fill entries. */
+    	tp->tx_full = 1;
+  	
+	if (entry == TX_RING_SIZE-1) 
+	  	flag = 0xe0000000 | DESC_RING_WRAP;
+
+  	tp->tx_ring[entry].length = cpu_to_le32(skb->len | flag);
+  	/* Pass ownership to the chip. */
+  	tp->tx_ring[entry].status = cpu_to_le32(DescOwned);
+  	tp->cur_tx++;
+
+#ifndef DEMAND_POLLTX
+  	outl(0, dev->base_addr + CSR1); 
+  	dev->trans_start = jiffies; 
+#endif
+	if (!tp->tx_full) 
+	    clear_bit(0, (void*)&dev->tbusy);
+
+  	return 0;
+}
+
+/* clean up tx dma ring */
+static int
+tulip_tx_clean(struct device *dev)
+{ 
+  	struct tulip_private *tp; 
+  	unsigned int dirty_tx; 
+	int ret;
+  	tp = (struct tulip_private *)dev->priv; 
+	
+  	for (dirty_tx = tp->dirty_tx; tp->cur_tx - dirty_tx > 0; dirty_tx++) {
+    	int entry = dirty_tx % TX_RING_SIZE; 
+    	int status = le32_to_cpu(tp->tx_ring[entry].status);
+#ifdef PREFETCH
+    	int next_entry = (dirty_tx+1) % TX_RING_SIZE;
+    	volatile int next_status;
+    	next_status = tp->tx_ring[next_entry].status;
+#endif
+    
+    	if (status < 0) break; /* It still hasn't been Txed */ 
+    
+    	/* Check for Rx filter setup frames. */ 
+    	if (tp->tx_skbuff[entry] == NULL) continue;
+
+    	if (status & 0x8000) { 
+      		/* There was an major error, log it. */
+      		tp->stats.tx_errors++; 
+      		if (status & 0x4104) tp->stats.tx_aborted_errors++; 
+      		if (status & 0x0C00) tp->stats.tx_carrier_errors++; 
+      		if (status & 0x0200) tp->stats.tx_window_errors++;
+      		if (status & 0x0002) tp->stats.tx_fifo_errors++;
+      		if ((status & 0x0080) && tp->full_duplex == 0) 
+				tp->stats.tx_heartbeat_errors++;
+    	} else {
+      		tp->stats.tx_bytes += tp->tx_ring[entry].length & 0x7ff; 
+      		tp->stats.collisions += (status >> 3) & 15; 
+      		tp->stats.tx_packets++; 
+    	}
+
+		tulip_recycle_or_free_skb(tp->tx_skbuff[entry]);
+      	tp->tx_skbuff[entry] = 0; 
+  	}
+
+  	if (tp->tx_full && dev->tbusy && tp->cur_tx-dirty_tx < TX_RING_SIZE-2) { 
+    	/* The ring is no longer full, clear tbusy. */ 
+    	tp->tx_full = 0; 
+		dev->tbusy = 0;
+  	}
+
+  	tp->dirty_tx = dirty_tx; 
+	ret = tp->cur_tx - tp->dirty_tx;
+	
+  	return ret;
+}
+

Index: drivers/net/acenic.c
--- drivers/net/acenic.c.orig	Wed Mar 29 11:03:02 2000
+++ drivers/net/acenic.c	Sat Apr  1 15:17:18 2000
@@ -276,6 +276,21 @@
 			printk(version);
 		}
 
+		/*
+		 * polling support 
+		 */
+		dev->pollable = 1;
+		dev->intr_is_on = 1;
+		dev->rx_dma_length = RX_RETURN_RING_ENTRIES;
+		dev->tx_dma_length = TX_RING_ENTRIES;
+		dev->rx_poll   = ace_rx_poll;
+        	dev->rx_refill = ace_rx_refill;
+        	dev->tx_clean  = ace_tx_clean;
+        	dev->tx_queue  = ace_tx_queue;
+        	dev->tx_start  = ace_tx_start;
+        	dev->intr_off  = ace_interrupt_off;
+        	dev->intr_on   = ace_interrupt_on;
+
 		pci_read_config_word(pdev, PCI_COMMAND, &ap->pci_command);
 
 		pci_read_config_byte(pdev, PCI_LATENCY_TIMER, &pci_latency);
@@ -1088,6 +1103,220 @@
 	return evtcsm;
 }
 
+/* this code is a modified version of ace_rx_int, changes to ace_rx_int should
+ * be reflected in here */
+struct sk_buff* ace_rx_poll(struct device *dev, int *want) 
+{
+	struct ace_private *ap = (struct ace_private *)dev->priv;
+	struct ace_regs *regs = ap->regs;
+	struct sk_buff *skb_head, *skb_last;
+	u32 idx, oldidx;
+	u32 rxretcsm, rxretprd;
+	int got = 0;
+  	
+	if (dev->intr_is_on) {
+	  printk(KERN_ERR "%s: polling procedure ace_rx_poll called while interrupt is still on\n", dev->name);
+	  return 0L;
+	}
+
+	spin_lock(&ap->lock);
+
+	skb_head = skb_last = NULL;
+	rxretprd = ap->rx_ret_prd;
+	rxretcsm = ap->cur_rx;
+	idx = rxretcsm;
+
+	while (idx != rxretprd && got < *want){
+		struct sk_buff *skb, *oldskb;
+		struct rx_desc *oldrxdesc;
+		u32 size;
+		u16 csum;
+		int jumbo;
+
+		oldidx = ap->rx_return_ring[idx].idx;
+		jumbo = ap->rx_return_ring[idx].flags & DFLG_RX_JUMBO;
+
+		if (jumbo){
+			oldskb = ap->rx_jumbo_skbuff[oldidx];
+			oldrxdesc = &ap->rx_jumbo_ring[oldidx];
+		}else{
+			oldskb = ap->rx_std_skbuff[oldidx];
+			oldrxdesc = &ap->rx_std_ring[oldidx];
+		}
+
+		size = oldrxdesc->size;
+
+		skb = oldskb;
+
+		skb_put(skb, size);
+		
+		if (jumbo){
+			ap->rx_jumbo_skbuff[oldidx] = NULL;
+		}else{
+			ap->rx_std_skbuff[oldidx] = NULL;
+		}
+
+		/*
+		 * Fly baby, fly!
+		 */
+		csum = ap->rx_return_ring[idx].tcp_udp_csum;
+
+		skb->dev = dev;
+		skb->protocol = eth_type_trans(skb, dev);
+
+#if 0
+		/*
+		 * This was never actually enabled in the RX descriptors
+		 * anyway - it requires a bit more testing before enabling
+		 * it again.
+		 */
+		/*
+		 * If the checksum is correct and this is not a
+		 * fragment, tell the stack that the data is correct.
+		 */
+		if(!(csum ^ 0xffff) &&
+		   (!(((struct iphdr *)skb->data)->frag_off &
+		      __constant_htons(IP_MF|IP_OFFSET))))
+			skb->ip_summed = CHECKSUM_UNNECESSARY;
+		else
+			skb->ip_summed = CHECKSUM_NONE;
+#endif
+		if (got == 0) {
+		  	skb_head = skb;
+			skb_last = skb;
+			skb_last->next = NULL;
+			skb_last->prev = NULL;
+		} else {
+		  	skb_last->next = skb;
+			skb->prev = skb_last;
+			skb->next = NULL;
+			skb_last = skb;
+		}
+		got++;
+
+		ap->stats.rx_packets++;
+		ap->stats.rx_bytes += skb->len;
+
+		idx = (idx + 1) % RX_RETURN_RING_ENTRIES;
+	}
+	
+	if (ap->version == 1) {
+		/*
+	 	 * According to the documentation RxRetCsm is obsolete with
+	 	 * the 12.3.x Firmware - my Tigon I NIC's seem to disagree!
+	 	 */
+		writel(idx, &regs->RxRetCsm);
+	}
+	ap->cur_rx = idx;
+	*want = got;
+	
+	spin_unlock(&ap->lock);
+
+	return skb_head;
+}
+
+/* this code is a modified version of ace_rx_int, changes to ace_rx_int should
+ * be reflected in here */
+static int ace_rx_refill_nolock(struct device *dev)
+{
+	struct ace_private *ap = (struct ace_private *)dev->priv;
+	struct ace_regs *regs = ap->regs;
+	u32 idx;
+
+	if (ap->dirty_rx != ap->cur_rx) {
+		struct sk_buff *newskb;
+		struct rx_desc *newrxdesc, *oldrxdesc;
+		u32 prdidx, size, oldidx;
+		void *addr;
+		int jumbo;
+
+		idx = ap->dirty_rx;
+		oldidx = ap->rx_return_ring[idx].idx;
+		jumbo = ap->rx_return_ring[idx].flags & DFLG_RX_JUMBO;
+
+		if (jumbo){
+			prdidx = ap->rx_jumbo_skbprd;
+			newrxdesc = &ap->rx_jumbo_ring[prdidx];
+			oldrxdesc = &ap->rx_jumbo_ring[oldidx];
+		}else{
+			prdidx = ap->rx_std_skbprd;
+			newrxdesc = &ap->rx_std_ring[prdidx];
+			oldrxdesc = &ap->rx_std_ring[oldidx];
+		}
+
+		size = oldrxdesc->size;
+
+		newskb = alloc_skb(size + 2, GFP_ATOMIC);
+		if (newskb == NULL){
+			printk(KERN_ERR "%s: Out of memory\n",
+			       dev->name);
+			return -1;
+		}
+
+		/*
+		 * Make sure we DMA directly into nicely
+		 * aligned receive buffers
+		 */
+		skb_reserve(newskb, 2);
+		addr = (void *)virt_to_bus(newskb->data);
+
+		set_aceaddr_bus(&newrxdesc->addr, addr);
+		newrxdesc->size = size;
+
+		newrxdesc->flags = oldrxdesc->flags;
+		newrxdesc->idx = prdidx;
+		newrxdesc->type = DESC_RX;
+#if (BITS_PER_LONG == 32)
+		newrxdesc->addr.addrhi = 0;
+#endif
+
+		oldrxdesc->size = 0;
+		set_aceaddr_bus(&oldrxdesc->addr, 0);
+
+		if (jumbo){
+			ap->rx_jumbo_skbuff[prdidx] = newskb;
+
+			prdidx = (prdidx + 1) % RX_JUMBO_RING_ENTRIES;
+			ap->rx_jumbo_skbprd = prdidx;
+		}else{
+			ap->rx_std_skbuff[prdidx] = newskb;
+
+			prdidx = (prdidx + 1) % RX_STD_RING_ENTRIES;
+			ap->rx_std_skbprd = prdidx;
+		}
+
+		if ((prdidx & 0x7) == 0){
+			struct cmd cmd;
+			if (jumbo)
+				cmd.evt = C_SET_RX_JUMBO_PRD_IDX;
+			else
+				cmd.evt = C_SET_RX_PRD_IDX;
+			cmd.code = 0;
+			cmd.idx = prdidx;
+			ace_issue_cmd(regs, &cmd);
+		}
+
+		idx = (idx + 1) % RX_RETURN_RING_ENTRIES;
+		ap->dirty_rx = idx;
+	}
+
+	return 0;
+}
+static int ace_rx_refill(struct device *dev)
+{
+  	int r;
+	struct ace_private *ap = (struct ace_private *)dev->priv;
+
+	if (dev->intr_is_on) {
+	  printk(KERN_ERR "%s: polling procedure ace_rx_refill called while interrupt is still on\n", dev->name);
+	  return -1;
+	}
+	
+	spin_lock(&ap->lock);
+  	r = ace_rx_refill_nolock(dev);
+	spin_unlock(&ap->lock);
+  	return r;
+}
 
 static int ace_rx_int(struct device *dev, u32 rxretprd, u32 rxretcsm)
 {
@@ -1097,6 +1326,8 @@
 
 	idx = rxretcsm;
 
+	ace_rx_refill_nolock(dev);
+
 	while (idx != rxretprd){
 		struct sk_buff *skb, *newskb, *oldskb;
 		struct rx_desc *newrxdesc, *oldrxdesc;
@@ -1233,6 +1464,7 @@
 	 * the 12.3.x Firmware - my Tigon I NIC's seem to disagree!
 	 */
 	writel(idx, &regs->RxRetCsm);
+	ap->dirty_rx = idx;
 	ap->cur_rx = idx;
 
 	return idx;
@@ -1241,44 +1473,10 @@
 	goto out;
 }
 
-
-static void ace_interrupt(int irq, void *dev_id, struct pt_regs *ptregs)
+static int ace_service_tx(struct device *dev, int polling)
 {
-	struct ace_private *ap;
-	struct ace_regs *regs;
-	struct device *dev = (struct device *)dev_id;
-	u32 txcsm, rxretcsm, rxretprd;
-	u32 evtcsm, evtprd;
-
-	ap = (struct ace_private *)dev->priv;
-	regs = ap->regs;
-
-	spin_lock(&ap->lock);
-
-	/*
-	 * In case of PCI shared interrupts or spurious interrupts,
-	 * we want to make sure it is actually our interrupt before
-	 * spending any time in here.
-	 */
-	if (!(readl(&regs->HostCtrl) & IN_INT)){
-		spin_unlock(&ap->lock);
-		return;
-	}
-
-	/*
-	 * Tell the card not to generate interrupts while we are in here.
-	 */
-	writel(1, &regs->Mb0Lo);
-
-	/*
-	 * Service RX ints before TX
-	 */
-	rxretprd = ap->rx_ret_prd;
-	rxretcsm = ap->cur_rx;
-
-	if (rxretprd != rxretcsm)
-		rxretprd = ace_rx_int(dev, rxretprd, rxretcsm);
-
+	struct ace_private *ap = (struct ace_private *)dev->priv;
+	u32 txcsm;
 	txcsm = ap->tx_csm;
 	if (txcsm != ap->tx_ret_csm) {
 		u32 idx = ap->tx_ret_csm;
@@ -1303,7 +1501,8 @@
 		    (((ap->tx_prd + 1) % TX_RING_ENTRIES) != txcsm)){
 			ap->tx_full = 0;
 			dev->tbusy = 0;
-			mark_bh(NET_BH);
+			if (!polling) 
+			  	mark_bh(NET_BH);
 
 			/*
 			 * TX ring is no longer full, aka the
@@ -1314,7 +1513,32 @@
 
 		ap->tx_ret_csm = txcsm;
 	}
+	txcsm = ap->tx_csm;
+	return (ap->tx_prd >= txcsm) ? ap->tx_prd - txcsm 
+	  			     : ap->tx_prd + TX_RING_ENTRIES - txcsm;
+}
 
+static int ace_tx_clean(struct device *dev)
+{
+  	int r;
+	struct ace_private *ap = (struct ace_private *)dev->priv;
+
+  	if (dev->intr_is_on) {
+	  printk(KERN_ERR "%s: polling procedure ace_tx_clean called while interrupt is still on\n", dev->name);
+	  return -1;
+	}
+	
+	spin_lock(&ap->lock);
+  	r = ace_service_tx(dev, 1);
+	spin_unlock(&ap->lock);
+  	return r;
+}
+
+static void ace_interrupt_evt(struct device *dev)
+{
+	struct ace_private *ap = (struct ace_private *)dev->priv;
+	u32 evtcsm, evtprd;
+	struct ace_regs *regs = ap->regs;
 	evtcsm = readl(&regs->EvtCsm);
 	evtprd = ap->evt_prd;
 
@@ -1323,11 +1547,105 @@
 	}
 
 	writel(evtcsm, &regs->EvtCsm);
+}
+
+static void ace_interrupt(int irq, void *dev_id, struct pt_regs *ptregs)
+{
+	struct ace_private *ap;
+	struct ace_regs *regs;
+	struct device *dev = (struct device *)dev_id;
+	u32 rxretcsm, rxretprd;
+
+	ap = (struct ace_private *)dev->priv;
+	regs = ap->regs;
+
+	spin_lock(&ap->lock);
+
+	/*
+	 * In case of PCI shared interrupts or spurious interrupts,
+	 * we want to make sure it is actually our interrupt before
+	 * spending any time in here.
+	 */
+	if (!(readl(&regs->HostCtrl) & IN_INT)){
+		spin_unlock(&ap->lock);
+		return;
+	}
+
+	/*
+	 * Tell the card not to generate interrupts while we are in here.
+	 */
+	writel(1, &regs->Mb0Lo);
+
+	/*
+	 * Service RX ints before TX
+	 */
+	rxretprd = ap->rx_ret_prd;
+	rxretcsm = ap->cur_rx;
+
+	if (rxretprd != rxretcsm)
+		rxretprd = ace_rx_int(dev, rxretprd, rxretcsm);
+	
+	ace_service_tx(dev, 0);
+	ace_interrupt_evt(dev);
+
+	writel(0, &regs->Mb0Lo);
+
+	spin_unlock(&ap->lock);
+}
+
+/* polling support */
+
+static void ace_interrupt_on(struct device *dev)
+{
+	struct ace_private *ap = (struct ace_private *)dev->priv;
+	struct ace_regs *regs = ap->regs;
+	spin_lock(&ap->lock);
+	dev->intr_is_on = 1;
 	writel(0, &regs->Mb0Lo);
+	spin_unlock(&ap->lock);
+}
 
+static void ace_interrupt_off(struct device *dev)
+{
+	struct ace_private *ap = (struct ace_private *)dev->priv;
+	struct ace_regs *regs = ap->regs;
+	spin_lock(&ap->lock);
+	writel(1, &regs->Mb0Lo);
+	dev->intr_is_on = 0;
 	spin_unlock(&ap->lock);
 }
 
+static int ace_tx_start(struct device *dev)
+{
+  	int r;
+	struct ace_private *ap = (struct ace_private *)dev->priv;
+  	
+	if (dev->intr_is_on) {
+	  printk(KERN_ERR "%s: polling procedure ace_tx_start called while interrupt is still on\n", dev->name);
+	  return -1;
+	}
+	
+	spin_lock(&ap->lock);
+	r = ace_xmit(0L, dev, 1);
+	spin_unlock(&ap->lock);
+	return r;
+}
+
+static int ace_tx_queue(struct sk_buff *skb, struct device *dev)
+{
+  	int r;
+	struct ace_private *ap = (struct ace_private *)dev->priv;
+
+  	if (dev->intr_is_on) {
+	  printk(KERN_ERR "%s: polling procedure ace_tx_queue called while interrupt is still on\n", dev->name);
+	  return -1;
+	}
+	
+	spin_lock(&ap->lock);
+  	r = ace_xmit(skb, dev, 0);
+	spin_unlock(&ap->lock);
+	return r;
+}
 
 static int ace_open(struct device *dev)
 {
@@ -1441,7 +1759,7 @@
 }
 
 
-static int ace_start_xmit(struct sk_buff *skb, struct device *dev)
+static int ace_xmit(struct sk_buff *skb, struct device *dev, int send)
 {
 	struct ace_private *ap = (struct ace_private *)dev->priv;
 	struct ace_regs *regs = ap->regs;
@@ -1453,19 +1771,23 @@
 
 	idx = ap->tx_prd;
 
-	ap->tx_skbuff[idx] = skb;
-	addr = virt_to_bus(skb->data);
+	if (skb != 0) { 	/* polling support */
+		ap->tx_skbuff[idx] = skb;
+		addr = virt_to_bus(skb->data);
 #if (BITS_PER_LONG == 64)
-	writel(addr >> 32, &ap->tx_ring[idx].addr.addrhi);
+		writel(addr >> 32, &ap->tx_ring[idx].addr.addrhi);
 #endif
-	writel(addr & 0xffffffff, &ap->tx_ring[idx].addr.addrlo);
-	flagsize = (skb->len << 16) | (DESC_END) ;
-	writel(flagsize, &ap->tx_ring[idx].flagsize);
-	mb();
-	idx = (idx + 1) % TX_RING_ENTRIES;
+		writel(addr & 0xffffffff, &ap->tx_ring[idx].addr.addrlo);
+		flagsize = (skb->len << 16) | (DESC_END) ;
+		writel(flagsize, &ap->tx_ring[idx].flagsize);
+		mb();
+		idx = (idx + 1) % TX_RING_ENTRIES;
+
+		ap->tx_prd = idx;
+	} 			/* polling support */
 
-	ap->tx_prd = idx;
-	writel(idx, &regs->TxPrd);
+	if (send) 
+	  	writel(idx, &regs->TxPrd);
 
 	if ((idx + 1) % TX_RING_ENTRIES == ap->tx_ret_csm){
 		ap->tx_full = 1;
@@ -1483,6 +1805,11 @@
 
 	dev->trans_start = jiffies;
 	return 0;
+}
+
+static int ace_start_xmit(struct sk_buff *skb, struct device *dev)
+{
+  	return ace_xmit(skb, dev, 1);
 }
 
 

Index: drivers/net/acenic.h
--- drivers/net/acenic.h.orig	Wed Mar 29 11:03:02 2000
+++ drivers/net/acenic.h	Sat Apr  1 13:17:24 2000
@@ -670,5 +670,15 @@
 static int ace_set_mac_addr(struct device *dev, void *p);
 static struct net_device_stats *ace_get_stats(struct device *dev);
 static u8 read_eeprom_byte(struct ace_regs *regs, unsigned long offset);
+/* polling device driver */
+static int  ace_xmit(struct sk_buff *skb, struct device *dev, int send);
+static struct sk_buff* ace_rx_poll(struct device *dev, int *want);
+static int  ace_rx_refill(struct device *dev);
+static int  ace_tx_clean(struct device *dev);
+static int  ace_tx_start(struct device *dev);
+static int  ace_tx_queue(struct sk_buff *skbuff, struct device *dev);
+static void ace_interrupt_evt(struct device *dev);
+static void ace_interrupt_on(struct device *dev);
+static void ace_interrupt_off(struct device *dev);
 
 #endif /* _ACENIC_H_ */

Index: include/linux/signal.h
--- include/linux/signal.h.orig	Thu Mar  2 18:27:42 2000
+++ include/linux/signal.h	Wed Apr  5 17:00:49 2000
@@ -189,7 +189,7 @@
 		memset(&set->sig[1], 0, sizeof(long)*(_NSIG_WORDS-1));
 		break;
 	case 2: set->sig[1] = 0;
-	case 1:
+	case 1: ;
 	}
 }
 
@@ -201,7 +201,7 @@
 		memset(&set->sig[1], -1, sizeof(long)*(_NSIG_WORDS-1));
 		break;
 	case 2: set->sig[1] = -1;
-	case 1:
+	case 1: ;
 	}
 }
 

Index: include/linux/skbuff.h
--- include/linux/skbuff.h.orig	Thu Mar  2 18:27:42 2000
+++ include/linux/skbuff.h	Sat Apr 22 15:31:21 2000
@@ -145,6 +145,7 @@
 extern struct sk_buff *		skb_peek_copy(struct sk_buff_head *list);
 extern struct sk_buff *		alloc_skb(unsigned int size, int priority);
 extern struct sk_buff *		dev_alloc_skb(unsigned int size);
+extern struct sk_buff *		skb_recycle(struct sk_buff *buf);
 extern void			kfree_skbmem(struct sk_buff *skb);
 extern struct sk_buff *		skb_clone(struct sk_buff *skb, int priority);
 extern struct sk_buff *		skb_copy(struct sk_buff *skb, int priority);
@@ -551,10 +552,15 @@
 extern __inline__ struct sk_buff *dev_alloc_skb(unsigned int length)
 {
 	struct sk_buff *skb;
+#if 1
+#define SKB_RESERVE_LENGTH 32
+#else
+#define SKB_RESERVE_LENGTH 16
+#endif
 
-	skb = alloc_skb(length+16, GFP_ATOMIC);
+	skb = alloc_skb(length+SKB_RESERVE_LENGTH, GFP_ATOMIC);
 	if (skb)
-		skb_reserve(skb,16);
+		skb_reserve(skb,SKB_RESERVE_LENGTH);
 	return skb;
 }
 
