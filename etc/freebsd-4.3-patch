
Index: net/if_var.h
--- net/if_var.h.orig	Sun Mar  4 16:09:45 2001
+++ net/if_var.h	Tue Aug 21 18:47:08 2001
@@ -137,6 +137,45 @@
 		__P((void *));
 	int	(*if_resolvemulti)	/* validate/resolve multicast */
 		__P((struct ifnet *, struct sockaddr **, struct sockaddr *));
+
+	/*
+	 * Modifications for Click, mostly polling support.
+	 */
+	int	(*if_click_poll_on)	/* enable polling */
+		__P((struct ifnet *));
+	int	(*if_click_poll_off)	/* disable polling */
+		__P((struct ifnet *));
+	struct mbuf *(*if_click_rx_poll)/* poll for received packets */
+		__P((struct ifnet *, int *want));
+	int	(*if_click_rx_refill)	/* refill the rx ring */
+		__P((struct ifnet *));
+	int	(*if_click_tx_queue)	/* queue a packet on the dma ring */
+		__P((struct ifnet *, struct mbuf *));
+	int	(*if_click_tx_clean)	/* clean the tx dma ring */
+		__P((struct ifnet *));
+	int	(*if_click_tx_start)	/* start transmitting */
+		__P((struct ifnet *));
+	int	(*if_click_tx_eob)	/* signal the end of a batch */
+		__P((struct ifnet *));
+	/*
+	 * Polling flag: negative if polling is not supported, zero
+	 * if polling supported but not enabled, and positive if
+	 * polling enabled (actual value reflects polling protocol
+	 * version.)
+	 */
+	int	polling;
+
+	/*
+	 * Packet diversion flag: if nonzero, incoming packets should
+	 * be queued onto click_intrq on this interface.
+	 */
+	int	click_divert;
+	struct ifqueue click_intrq;
+
+	/*
+	 * End of Click polling additions
+	 */
+
 	struct	ifqueue if_snd;		/* output queue */
 	struct	ifqueue *if_poll_slowq;	/* input queue for slow devices */
 	struct	ifprefixhead if_prefixhead; /* list of prefixes per if */

Index: net/if_ethersubr.c
--- net/if_ethersubr.c.orig	Tue Mar 13 17:00:32 2001
+++ net/if_ethersubr.c	Tue Aug 21 18:47:08 2001
@@ -500,6 +500,22 @@
 	register struct llc *l;
 #endif
 
+    if (ifp->click_divert) {
+	/*
+	 * Put the packet on Click's receive queue.  Click will
+	 * pick them up from the queue itself.
+	 */
+	inq = &ifp->click_intrq;
+	s = splimp();
+	if (IF_QFULL(inq)) {
+		IF_DROP(inq);
+		m_freem(m);
+	} else
+		IF_ENQUEUE(inq, m);
+	splx(s);
+	return;
+    }
+
 #ifdef BRIDGE
     if (! (do_bridge && BDG_USED(ifp) ) )
 #endif

Index: pci/if_fxpvar.h
--- pci/if_fxpvar.h.orig	Tue Oct 24 17:47:36 2000
+++ pci/if_fxpvar.h	Tue Aug 21 18:47:08 2001
@@ -53,6 +53,7 @@
 	bus_space_handle_t sc_sh;	/* bus space handle */
 	struct mbuf *rfa_headm;		/* first mbuf in receive frame area */
 	struct mbuf *rfa_tailm;		/* last mbuf in receive frame area */
+	int rfa_bufs;			/* number of rfa's allocated */
 	struct fxp_cb_tx *cbl_first;	/* first active TxCB in list */
 	int tx_queued;			/* # of active TxCB's */
 	int need_mcsetup;		/* multicast filter needs programming */

Index: pci/if_fxpreg.h
--- pci/if_fxpreg.h.orig	Wed Oct  4 06:16:12 2000
+++ pci/if_fxpreg.h	Tue Aug 21 18:47:08 2001
@@ -97,6 +97,9 @@
 #define FXP_SCB_COMMAND_RU_BASE		6
 #define FXP_SCB_COMMAND_RU_RBDRESUME	7
 
+#define FXP_SCB_INTRCNTL_ENABLE		0
+#define FXP_SCB_INTRCNTL_DISABLE	1
+
 /*
  * Command block definitions
  */

Index: pci/if_fxp.c
--- pci/if_fxp.c.orig	Tue Oct 24 17:47:36 2000
+++ pci/if_fxp.c	Tue Aug 21 18:47:08 2001
@@ -237,6 +237,15 @@
 static void fxp_stats_update	__P((void *));
 static void fxp_mc_setup	__P((struct fxp_softc *));
 
+static int fxp_poll_on		__P((struct ifnet *));
+static int fxp_poll_off		__P((struct ifnet *));
+static struct mbuf *fxp_rx_poll	__P((struct ifnet *, int *want));
+static int fxp_rx_refill	__P((struct ifnet *));
+static int fxp_tx_queue		__P((struct ifnet *, struct mbuf *));
+static int fxp_tx_clean		__P((struct ifnet *));
+static int fxp_tx_start		__P((struct ifnet *));
+static int fxp_tx_eob		__P((struct ifnet *));
+
 /*
  * Set initial transmit threshold at 64 (512 bytes). This is
  * increased by 64 (512 bytes) at a time, to maximum of 192
@@ -616,6 +625,19 @@
 	ifp->if_watchdog = fxp_watchdog;
 
 	/*
+	 * Set up for Click polling support
+	 */
+	ifp->if_click_poll_on = fxp_poll_on;
+	ifp->if_click_poll_off = fxp_poll_off;
+	ifp->if_click_rx_poll = fxp_rx_poll;
+	ifp->if_click_rx_refill = fxp_rx_refill;
+	ifp->if_click_tx_queue = fxp_tx_queue;
+	ifp->if_click_tx_clean = fxp_tx_clean;
+	ifp->if_click_tx_start = fxp_tx_start;
+	ifp->if_click_tx_eob = fxp_tx_eob;
+	ifp->polling = 0;
+
+	/*
 	 * Attach the interface.
 	 */
 	ether_ifattach(ifp, ETHER_BPF_SUPPORTED);
@@ -836,6 +858,7 @@
 	/*
 	 * Pre-allocate our receive buffers.
 	 */
+	sc->rfa_bufs = 0;
 	for (i = 0; i < FXP_NRFABUFS; i++) {
 		if (fxp_add_rfabuf(sc, NULL) != 0) {
 			goto fail;
@@ -1206,6 +1229,15 @@
 		return;
 	}
 
+	if (ifp->polling > 0) {
+		/*
+		 * We shouldn't receive any interrupts while in
+		 * polling mode, so simply ignore the interrupt.
+		 * It could be due to PCI IRQ sharing..
+		 */
+		return;
+	}
+
 	while ((statack = CSR_READ_1(sc, FXP_CSR_SCB_STATACK)) != 0) {
 #if defined(__NetBSD__)
 		claimed = 1;
@@ -1261,6 +1293,7 @@
 				 */
 				sc->rfa_headm = m->m_next;
 				m->m_next = NULL;
+				sc->rfa_bufs--;
 
 				/*
 				 * Add a new buffer to the receive chain.
@@ -1463,6 +1496,7 @@
 		m_freem(sc->rfa_headm);
 	sc->rfa_headm = NULL;
 	sc->rfa_tailm = NULL;
+	sc->rfa_bufs = 0;
 	for (i = 0; i < FXP_NRFABUFS; i++) {
 		if (fxp_add_rfabuf(sc, NULL) != 0) {
 			/*
@@ -1487,6 +1521,9 @@
 {
 	struct fxp_softc *sc = ifp->if_softc;
 
+	if (ifp->polling > 0)
+		return;
+
 	printf(FXP_FORMAT ": device timeout\n", FXP_ARGS(sc));
 	ifp->if_oerrors++;
 
@@ -1886,6 +1923,7 @@
 	} else {
 		sc->rfa_headm = m;
 	}
+	sc->rfa_bufs++;
 	sc->rfa_tailm = m;
 
 	return (m == oldm);
@@ -2150,4 +2188,284 @@
 
 	ifp->if_timer = 2;
 	return;
+}
+
+/*
+ * Click polling code, mostly borrowed from the interrupt handling
+ * routines above, with some performance modifications.
+ *
+ * Currently, no buffer reuse is done: buffers are allocated in
+ * the driver in rx_refill and freed in tx_clean.  Probably some
+ * performance measurement is necessary to see whether it makes
+ * sense to save buffers manually.
+ */
+
+static struct mbuf *fxp_first_free_m(struct mbuf *m) {
+	while (m) {
+		struct fxp_rfa *rfa = (struct fxp_rfa *)
+						(m->m_ext.ext_buf +
+						 RFA_ALIGNMENT_FUDGE);
+
+		if (!(rfa->rfa_status & FXP_RFA_STATUS_C))
+			break;
+		m = m->m_next;
+	}
+
+	return m;
+}
+
+static int fxp_poll_on(struct ifnet *ifp) {
+	struct fxp_softc *sc = ifp->if_softc;
+
+	if (ifp->polling == 0) {
+		CSR_WRITE_1(sc, FXP_CSR_SCB_INTRCNTL, FXP_SCB_INTRCNTL_DISABLE);
+		ifp->polling = 2;
+	}
+
+	return 0;
+}
+
+static int fxp_poll_off(struct ifnet *ifp) {
+	struct fxp_softc *sc = ifp->if_softc;
+
+	if (ifp->polling > 0) {
+		CSR_WRITE_1(sc, FXP_CSR_SCB_INTRCNTL, FXP_SCB_INTRCNTL_ENABLE);
+		ifp->polling = 0;
+	}
+
+	return 0;
+}
+
+static int fxp_ru_check(struct fxp_softc *sc, struct mbuf *last_m) {
+	struct fxp_rfa *rfa;
+	struct mbuf *m = fxp_first_free_m(last_m);
+	int statack;
+
+	/*
+	 * Optimization: don't read registers unless we've got
+	 * an empty RFA which we can feed to the RU..
+	 */
+	if (m) {
+		statack = CSR_READ_1(sc, FXP_CSR_SCB_STATACK);
+		CSR_WRITE_1(sc, FXP_CSR_SCB_STATACK, statack);
+	}
+
+	if (statack & FXP_SCB_STATACK_RNR) {
+		/*
+		 * Recompute the last free mbuf to avoid race
+		 * conditions above.  At this point, we know
+		 * the card is stalled and shouldn't be DMAing
+		 * to the RFA.
+		 */
+		m = fxp_first_free_m(m);
+		if (!m) {
+			/*
+			 * We've acknowledged the RNR condition
+			 * but all the RFAs are already in use.
+			 * So we must reload the card with some
+			 * buffer..  Blow away the last packet.
+			 */
+			m = last_m;
+			while (m && m->m_next)
+				m = m->m_next;
+
+			rfa = (struct fxp_rfa *) (m->m_ext.ext_buf +
+						  RFA_ALIGNMENT_FUDGE);
+			rfa->rfa_status = 0;
+		} else {
+			rfa = (struct fxp_rfa *) (m->m_ext.ext_buf +
+						  RFA_ALIGNMENT_FUDGE);
+		}
+
+		printf(FXP_FORMAT ": Rx stall, restarting\n",
+		       FXP_ARGS(sc));
+		if (rfa->rfa_status & FXP_RFA_STATUS_C)
+			printf(FXP_FORMAT ": RFA inconsistency!\n",
+			       FXP_ARGS(sc));
+		fxp_scb_wait(sc);
+		CSR_WRITE_4(sc, FXP_CSR_SCB_GENERAL,
+			    vtophys(rfa));
+		CSR_WRITE_1(sc, FXP_CSR_SCB_COMMAND,
+			    FXP_SCB_COMMAND_RU_START);
+		return 1;
+	}
+
+	return 0;
+}
+
+static struct mbuf *fxp_rx_poll(struct ifnet *ifp, int *want) {
+	struct fxp_softc *sc = ifp->if_softc;
+	int need = *want, got = 0;
+	struct mbuf *m, *m_list = NULL;
+	struct fxp_rfa *rfa;
+
+	/*
+	 * -- Check that there's a receive buffer to be collected.
+	 * -- Check that it's not the last buffer (in which case we
+	 *    can't use it: the card needs to DMA the next pointer
+	 *    out of this RFA!
+	 * -- Check if we've already collected enough packets.
+	 * -- Check that this RFA has been completed (ie the card
+	 *    has DMA'd something into this buffer.)
+	 */
+	while ((m = sc->rfa_headm) &&
+	       (m->m_next) &&
+	       (rfa = (struct fxp_rfa *)(m->m_ext.ext_buf +
+					 RFA_ALIGNMENT_FUDGE)) &&
+	       (got < need) &&
+	       (rfa->rfa_status & FXP_RFA_STATUS_C))
+	{
+		struct ether_header *eh;
+		int total_len;
+
+		/*
+		 * Remove first packet from the chain.
+		 */
+		sc->rfa_headm = m->m_next;
+		sc->rfa_bufs--;
+		m->m_next = NULL;
+
+		/*
+		 * Compute the actual size of the packet,
+		 * verify that it's sane.
+		 */
+		total_len = rfa->actual_size & (MCLBYTES - 1);
+		if (total_len <
+		    sizeof(struct ether_header)) {
+			m_freem(m);
+			continue;
+		}
+
+		/*
+		 * Write out the mbuf packet header
+		 */
+		m->m_pkthdr.rcvif = ifp;
+		m->m_pkthdr.len = m->m_len = total_len;
+		eh = mtod(m, struct ether_header *);
+		m->m_data += sizeof(struct ether_header);
+		m->m_len -= sizeof(struct ether_header);
+		m->m_pkthdr.len = m->m_len;
+
+		/*
+		 * Tack this mbuf onto our return list.
+		 */
+		m->m_nextpkt = m_list;
+		m_list = m;
+		got++;
+	}
+
+	/*
+	 * If we weren't able to receive any packets, and the queue
+	 * might have free packets, check the RU status.
+	 */
+	if (got == 0 && sc->rfa_headm && sc->rfa_headm->m_next)
+		fxp_ru_check(sc, sc->rfa_headm);
+
+	*want = got;
+	return m_list;
+}
+
+static int fxp_rx_refill(struct ifnet *ifp) {
+	struct fxp_softc *sc = ifp->if_softc;
+	struct mbuf *last_m;
+	int check_ru_stall = 0;
+
+	/*
+	 * Because rfa_tailm isn't always kept up-to-date, we only
+	 * use it if rfa_headm is valid (otherwise there aren't any
+	 * buffers in the RX DMA ring and the tail is null too.)
+	 */
+	last_m = sc->rfa_headm ? sc->rfa_tailm : NULL;
+
+	/*
+	 * Actually do the refill by allocating buffers until we have
+	 * enough (FXP_NRFABUFS).
+	 */
+	while (sc->rfa_bufs < FXP_NRFABUFS) {
+		int ret;
+
+		ret = fxp_add_rfabuf(sc, NULL);
+		/*
+		 * If we couldn't allocate more buffers,
+		 * exit, and print a warning.
+		 */
+		if (ret) {
+			printf(FXP_FORMAT ": Unable to get buffers in refill\n",
+			       FXP_ARGS(sc));
+			break;
+		}
+	}
+
+	/*
+	 * If there's no buffers on the RX ring, or if the last
+	 * buffer is already completed, we must check for an RU
+	 * stall.
+	 */
+	if (!last_m) {
+		check_ru_stall = 1;
+	} else {
+		struct fxp_rfa *rfa;
+
+		rfa = (struct fxp_rfa *)(last_m->m_ext.ext_buf +
+					 RFA_ALIGNMENT_FUDGE);
+		if (rfa->rfa_status & FXP_RFA_STATUS_C)
+			check_ru_stall = 1;
+	}
+
+	/*
+	 * If we didn't have any buffers at all to begin with,
+	 * the first newly-allocated buffer is now in rfa_headm,
+	 * otherwise it's in last_m->m_next.
+	 *
+	 * From this point, last_m points at the first buffer
+	 * we've just allocated (if any).
+	 */
+	if (last_m)
+		last_m = last_m->m_next;
+	else
+		last_m = sc->rfa_headm;
+
+	/*
+	 * If we have succeeded in allocating some receive buffers,
+	 * and we need to check the RU, do so now.
+	 */
+	if (check_ru_stall && last_m)
+		fxp_ru_check(sc, last_m);
+
+	return 0;
+}
+
+static int fxp_tx_queue(struct ifnet *ifp, struct mbuf *m) {
+	return 0;
+}
+
+static int fxp_tx_clean(struct ifnet *ifp) {
+	return 0;
+}
+
+static int fxp_tx_start(struct ifnet *ifp) {
+	struct fxp_softc *sc = ifp->if_softc;
+
+	/*
+	 * This should only be called if the Tx appears to be
+	 * stalling.  So, just wake up the command unit for
+	 * now.
+	 */
+	fxp_scb_wait(sc);
+	CSR_WRITE_1(sc, FXP_CSR_SCB_COMMAND, FXP_SCB_COMMAND_CU_RESUME);
+
+	return 0;
+}
+
+static int fxp_tx_eob(struct ifnet *ifp) {
+	struct fxp_softc *sc = ifp->if_softc;
+
+	/*
+	 * Wake up the command unit to transmit the newly chained
+	 * entries on the Tx list.
+	 */
+	fxp_scb_wait(sc);
+	CSR_WRITE_1(sc, FXP_CSR_SCB_COMMAND, FXP_SCB_COMMAND_CU_RESUME);
+
+	return 0;
 }

Index: pci/if_wxvar.h
--- pci/if_wxvar.h.orig	Wed Aug 22 12:59:33 2001
+++ pci/if_wxvar.h	Wed Aug 22 12:59:41 2001
@@ -176,8 +176,8 @@
 #endif
 
 struct wxmdvar {
-	struct device *		dev;	/* backpointer to device */
 	struct arpcom 		arpcom;	/* per-interface network data */
+	struct device *		dev;	/* backpointer to device */
 	struct resource *	mem;	/* resource descriptor for registers */
 	struct resource *	irq;	/* resource descriptor for interrupt */
 	void *			ih;	/* interrupt handler cookie */

Index: pci/if_wxreg.h
--- pci/if_wxreg.h.orig	Thu Aug 23 15:16:01 2001
+++ pci/if_wxreg.h	Thu Aug 23 15:53:28 2001
@@ -115,6 +115,23 @@
  * Register access via offsets.
  */
 
+#define	WXREG_LVG_SELECT(rn)	(IS_LIVENGOOD(sc) ? WXREG_LVG_##rn : \
+						    WXREG_WSM_##rn)
+#define	WXREG_RDTR0		WXREG_LVG_SELECT(RDTR0)
+#define	WXREG_RDBA0_LO		WXREG_LVG_SELECT(RDBA0_LO)
+#define	WXREG_RDBA0_HI		WXREG_LVG_SELECT(RDBA0_HI)
+#define	WXREG_RDLEN0		WXREG_LVG_SELECT(RDLEN0)
+#define	WXREG_RDH0		WXREG_LVG_SELECT(RDH0)
+#define	WXREG_RDT0		WXREG_LVG_SELECT(RDT0)
+#define	WXREG_FLOW_RCV_HI	WXREG_LVG_SELECT(FLOW_RCV_HI)
+#define	WXREG_FLOW_RCV_LO	WXREG_LVG_SELECT(FLOW_RCV_LO)
+#define	WXREG_TDLEN		WXREG_LVG_SELECT(TDLEN)
+#define	WXREG_TDBA_LO		WXREG_LVG_SELECT(TDBA_LO)
+#define	WXREG_TDBA_HI		WXREG_LVG_SELECT(TDBA_HI)
+#define	WXREG_TDH		WXREG_LVG_SELECT(TDH)
+#define	WXREG_TDT		WXREG_LVG_SELECT(TDT)
+#define	WXREG_TIDV		WXREG_LVG_SELECT(TIDV)
+
 #define	WXREG_DCR		0x00000000
 #define	WXREG_DSR		0x00000008
 #define	WXREG_EECDR		0x00000010
@@ -132,20 +149,20 @@
 #define	WXREG_IMASK		0x000000d0
 #define	WXREG_IMCLR		0x000000d8
 #define	WXREG_RCTL		0x00000100
-#define	WXREG_RDTR0		0x00000108
-#define	WXREG_RDBA0_LO		0x00000110
-#define	WXREG_RDBA0_HI		0x00000114
-#define	WXREG_RDLEN0		0x00000118
-#define	WXREG_RDH0		0x00000120
-#define	WXREG_RDT0		0x00000128
+#define	WXREG_WSM_RDTR0		0x00000108
+#define	WXREG_WSM_RDBA0_LO	0x00000110
+#define	WXREG_WSM_RDBA0_HI	0x00000114
+#define	WXREG_WSM_RDLEN0	0x00000118
+#define	WXREG_WSM_RDH0		0x00000120
+#define	WXREG_WSM_RDT0		0x00000128
 #define	WXREG_RDTR1		0x00000130
 #define	WXREG_RDBA1_LO		0x00000138
 #define	WXREG_RDBA1_HI		0x0000013C
 #define	WXREG_RDLEN1		0x00000140
 #define	WXREG_RDH1		0x00000148
 #define	WXREG_RDT1		0x00000150
-#define	WXREG_FLOW_RCV_HI	0x00000160
-#define	WXREG_FLOW_RCV_LO	0x00000168
+#define	WXREG_WSM_FLOW_RCV_HI	0x00000160
+#define	WXREG_WSM_FLOW_RCV_LO	0x00000168
 #define	WXREG_FLOW_XTIMER	0x00000170
 #define	WXREG_XMIT_CFGW		0x00000178
 #define	WXREG_RECV_CFGW		0x00000180
@@ -155,13 +172,28 @@
 #define	WXREG_TQSA_HI		0x0000040C
 #define	WXREG_TIPG		0x00000410
 #define	WXREG_TQC		0x00000418
-#define	WXREG_TDBA_LO		0x00000420
-#define	WXREG_TDBA_HI		0x00000424
-#define	WXREG_TDLEN		0x00000428
-#define	WXREG_TDH		0x00000430
-#define	WXREG_TDT		0x00000438
-#define	WXREG_TIDV		0x00000440
+#define	WXREG_WSM_TDBA_LO	0x00000420
+#define	WXREG_WSM_TDBA_HI	0x00000424
+#define	WXREG_WSM_TDLEN		0x00000428
+#define	WXREG_WSM_TDH		0x00000430
+#define	WXREG_WSM_TDT		0x00000438
+#define	WXREG_WSM_TIDV		0x00000440
 #define	WXREG_VFTA		0x00000600
+
+#define	WXREG_LVG_FLOW_RCV_LO	0x00002160
+#define	WXREG_LVG_FLOW_RCV_HI	0x00002168
+#define	WXREG_LVG_RDBA0_LO	0x00002800
+#define	WXREG_LVG_RDBA0_HI	0x00002804
+#define	WXREG_LVG_RDLEN0	0x00002808
+#define	WXREG_LVG_RDH0		0x00002810
+#define	WXREG_LVG_RDT0		0x00002818
+#define	WXREG_LVG_RDTR0		0x00002820
+#define	WXREG_LVG_TDBA_LO	0x00003800
+#define	WXREG_LVG_TDBA_HI	0x00003804
+#define	WXREG_LVG_TDLEN		0x00003808
+#define	WXREG_LVG_TDH		0x00003810
+#define	WXREG_LVG_TDT		0x00003818
+#define	WXREG_LVG_TIDV		0x00003820
 
 #define	WX_RAL_TAB_SIZE		16
 #define	WX_RAL_AV		0x80000000

Index: pci/if_wx.c
--- pci/if_wx.c.orig	Wed Aug 22 13:17:28 2001
+++ pci/if_wx.c	Thu Aug 23 15:40:56 2001
@@ -1087,7 +1087,7 @@
 
 			/*
 			 * If this packet is too small for the chip's minimum,
-			 * break out to to cluster it.
+			 * break out to cluster it.
 			 */
 			if (m->m_len < WX_MIN_RPKT_SIZE) {
 				sc->wx_xmitrunt++;

Index: sys/namei.h
--- sys/namei.h.orig	Fri Nov  3 10:55:40 2000
+++ sys/namei.h	Tue Aug 21 18:47:08 2001
@@ -40,6 +40,23 @@
 #include <sys/queue.h>
 #include <sys/uio.h>
 
+struct componentname {
+	/*
+	 * Arguments to lookup.
+	 */
+	u_long	cn_nameiop;	/* namei operation */
+	u_long	cn_flags;	/* flags to namei */
+	struct	proc *cn_proc;	/* process requesting lookup */
+	struct	ucred *cn_cred;	/* credentials */
+	/*
+	 * Shared between lookup and commit routines.
+	 */
+	char	*cn_pnbuf;	/* pathname buffer */
+	char	*cn_nameptr;	/* pointer to looked up name */
+	long	cn_namelen;	/* length of looked up component */
+	long	cn_consume;	/* chars to consume in lookup() */
+};
+
 /*
  * Encapsulation of namei parameters.
  */
@@ -75,22 +92,7 @@
 	 * information from the nameidata structure that is passed
 	 * through the VOP interface.
 	 */
-	struct componentname {
-		/*
-		 * Arguments to lookup.
-		 */
-		u_long	cn_nameiop;	/* namei operation */
-		u_long	cn_flags;	/* flags to namei */
-		struct	proc *cn_proc;	/* process requesting lookup */
-		struct	ucred *cn_cred;	/* credentials */
-		/*
-		 * Shared between lookup and commit routines.
-		 */
-		char	*cn_pnbuf;	/* pathname buffer */
-		char	*cn_nameptr;	/* pointer to looked up name */
-		long	cn_namelen;	/* length of looked up component */
-		long	cn_consume;	/* chars to consume in lookup() */
-	} ni_cnd;
+	struct componentname ni_cnd;
 };
 
 #ifdef _KERNEL
@@ -148,12 +150,12 @@
 static void NDINIT __P((struct nameidata *, u_long, u_long, enum uio_seg,
 	    const char *, struct proc *));
 static __inline void
-NDINIT(ndp, op, flags, segflg, namep, p)
-	struct nameidata *ndp;
-	u_long op, flags;
-	enum uio_seg segflg;
-	const char *namep;
-	struct proc *p;
+NDINIT(struct nameidata *ndp,
+       u_long op,
+       u_long flags,
+       enum uio_seg segflg,
+       const char *namep,
+       struct proc *p)
 {
 	ndp->ni_cnd.cn_nameiop = op;
 	ndp->ni_cnd.cn_flags = flags;

Index: sys/queue.h
--- sys/queue.h.orig	Tue Aug 21 20:18:22 2001
+++ sys/queue.h	Tue Aug 21 20:19:33 2001
@@ -516,7 +516,8 @@
 static __inline void
 insque(void *a, void *b)
 {
-	struct quehead *element = a, *head = b;
+	struct quehead *element = (struct quehead *) a,
+		       *head = (struct quehead *) b;
 
 	element->qh_link = head->qh_link;
 	element->qh_rlink = head;
@@ -527,7 +528,7 @@
 static __inline void
 remque(void *a)
 {
-	struct quehead *element = a;
+	struct quehead *element = (struct quehead *) a;
 
 	element->qh_link->qh_rlink = element->qh_rlink;
 	element->qh_rlink->qh_link = element->qh_link;

Index: sys/libkern.h
--- sys/libkern.h.orig	Tue Aug 21 20:19:41 2001
+++ sys/libkern.h	Tue Aug 21 20:19:51 2001
@@ -103,7 +103,7 @@
 	if (c == 0)
 		bzero(b, len);
 	else
-		for (bb = b; len--; )
+		for (bb = (char *) b; len--; )
 			*bb++ = c;
 	return (b);
 }
